[
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Setup: How to setup and run the tutorial",
    "section": "",
    "text": "Download notebooks from the right sidebar. Select Colab if you are using Google Colab."
  },
  {
    "objectID": "setup.html#download-notebooks",
    "href": "setup.html#download-notebooks",
    "title": "Setup: How to setup and run the tutorial",
    "section": "",
    "text": "Download notebooks from the right sidebar. Select Colab if you are using Google Colab."
  },
  {
    "objectID": "setup.html#computational-environments",
    "href": "setup.html#computational-environments",
    "title": "Setup: How to setup and run the tutorial",
    "section": "Computational environments",
    "text": "Computational environments\n\nJupyterHub at UHH\n\nOpen https://code.min.uni-hamburg.de/\nSelect File &gt; New Launcher\nAt the bottom, select Terminal\nIn the Terminal, enter:\ngit clone https://github.com/lnnrtwttkhn/rdm-datalad\ncd rdm-datalad\nmake install\nFrom the sidebar on the left, you can now open the tutorial notebook in the rdm-datalad folder:\n\ntutorial.ipynb\n\nPaste the installation command into the first cell of the notebook:\nsource .venv/bin/activate\n\n\n\nGoogle Colab\n\nOpen https://colab.research.google.com/\nSign in with a Google account\nSelect File &gt; Open notebook\nSelect Upload and open or drag the notebook file\nPaste the installation command into the first cell of the notebook:\n\n!apt-get install tree\npip install bash-kernel&gt;=0.10.0 datalad&gt;=1.3.1 ipykernel&gt;=7.1.0 nbclient&gt;=0.10.4 nbformat&gt;=5.10.4 pandas&gt;=3.0.0 scikit-learn&gt;=1.8.0 seaborn&gt;=0.13.2\n\n\nTerminal\n\nOpen a Terminal and copy-paste all commands in the tutorial"
  },
  {
    "objectID": "tutorial.html",
    "href": "tutorial.html",
    "title": "Version Control of Data with DataLad",
    "section": "",
    "text": "This tutorial was initially created by Adina Wagner for the 2020 OHBM Brainhack Traintrack session on DataLad. This notebook accompanies this tutorial video by Adina Wagner."
  },
  {
    "objectID": "tutorial.html#acknowledgements",
    "href": "tutorial.html#acknowledgements",
    "title": "Version Control of Data with DataLad",
    "section": "",
    "text": "This tutorial was initially created by Adina Wagner for the 2020 OHBM Brainhack Traintrack session on DataLad. This notebook accompanies this tutorial video by Adina Wagner."
  },
  {
    "objectID": "tutorial.html#introduction-setup",
    "href": "tutorial.html#introduction-setup",
    "title": "Version Control of Data with DataLad",
    "section": "Introduction & setup",
    "text": "Introduction & setup\nDataLad is a data management multitool that can assist you in handling the entire life cycle of digital objects. It is a command-line tool, free and open source, and available for all major operating systems. In the command line, all operations begin with the general datalad command.\n\n# datalad\n\nFor example, I can type datalad --help to find out more about the available commands.\nYou can find more details about how to install DataLad and its dependencies on all operating systems in the DataLad Handbook. This section also explains how to install DataLad on shared machines where you may not have administrative privileges (sudo rights), such as high-performance computing clusters. If you already have DataLad installed, make sure that it is a recent version. You can check the installed version using the datalad --version command:\n\ndatalad --version\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\ndatalad 1.3.1\n\n\n\nConfiguring your Git identity\nThe first step, if you haven’t done so already, is to configure your Git identity. If you’re new to Git, don’t worry! This configuration simply involves setting your name and email address, which will associate your changes in a project with you as the author.\nBelow, we provide instructions on how to configure your Git identity. However, please note that a central Git configuration has already been set up for you during the postBuild process of Binder, so you don’t need to perform this step manually. That’s why the code examples below is commented out.\n\n# git config user.name \"Your name\"\n# git config user.email \"Your email address\""
  },
  {
    "objectID": "tutorial.html#creating-a-datalad-dataset",
    "href": "tutorial.html#creating-a-datalad-dataset",
    "title": "Version Control of Data with DataLad",
    "section": "Creating a DataLad dataset",
    "text": "Creating a DataLad dataset\nEvery command in DataLad affects or uses DataLad datasets, the core data structure of DataLad. A dataset is a directory on a computer that DataLad manages. You can create new, empty datasets from scratch and populate them, or transform existing directories into datasets.\n\nLet’s start by creating a new DataLad dataset. Creating a new dataset is accomplished using the datalad create command. This command requires only a name for the dataset. It will then create a new directory with that name and instruct DataLad to manage it.\nAdditionally, the command includes an option, -c text2git. The -c option allows for specific configurations of the dataset at the time of creation. You can find detailed information about the text2git configuration in the DataLad handbook, specifically in the sections on configurations and procedures. Dont’t worry about configurations for now. In general, this configuration serves as a very useful standard for datasets.\n\ndatalad create -c text2git DataLad-101\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\nCommandError: 'git -c diff.ignoreSubmodules=none -c core.quotepath=false ls-files --stage -z -- DataLad-101' failed with exitcode 128\nfatal: detected dubious ownership in repository at '/app'\nTo add an exception for this directory, call:\n\n    git config --global --add safe.directory /app\n\n\n: 128\n\n\nRight after dataset creation, there is a new directory on the computer called DataLad-101. Let’s navigate into this directory using the cd command and list the directory contents using ls.\n\ncd DataLad-101\n\nbash: cd: DataLad-101: No such file or directory\n\n\n: 1\n\n\n\nls # ls does not show any output, because the dataset is empty\n\nDockerfile     _quarto-ipynb.yml    index.qmd\nLICENSE        _quarto.yml      index.quarto_ipynb\nMakefile       _site        pyproject.toml\nREADME.md      _variables.yml   setup.qmd\n_colab_install.md  binder       tutorial.qmd\n_extensions    convert_colab.py tutorial.quarto_ipynb\n_quarto-ci.yml     generate_install.py  uv.lock\n\n\nDatasets have the exciting feature of recording everything done within them. They provide version control for all content managed by DataLad, regardless of its size. Additionally, datasets maintain a complete history that you can interact with. This history is already present, although it is quite short at this point in time. Let’s take a look at it nonetheless. This history exists thanks to Git. You can access the history of a dataset using any tool that displays Git history. For simplicity, we will use Git’s built-in git log command.\n\ngit log\n\nfatal: detected dubious ownership in repository at '/app'\nTo add an exception for this directory, call:\n\n    git config --global --add safe.directory /app\n\n\n: 128"
  },
  {
    "objectID": "tutorial.html#version-control-workflows",
    "href": "tutorial.html#version-control-workflows",
    "title": "Version Control of Data with DataLad",
    "section": "Version control workflows",
    "text": "Version control workflows\nBuilding on top of Git and git-annex, DataLad allows you to version control arbitrarily large files in datasets. You can keep track of revisions of data of any size, and view, interact with or restore any version of your dataset’s history.\n\nLet’s start by creating a books directory using the mkdir command. Next, we will download two books from the internet. Here, we are using the command line tool curl to accomplish this, allowing us to perform all actions from the command line. However, if you prefer, you can also download the books manually and save them into the dataset using a file manager. Remember, a dataset is simply a directory on your computer.\n\nmkdir books\n\n\ncd books\n\n\ncurl -L -o TLCL.pdf https://sourceforge.net/projects/linuxcommand/files/TLCL/19.01/TLCL-19.01.pdf/download\ncurl -L -o byte-of-python.pdf https://edisciplinas.usp.br/pluginfile.php/3252353/mod_resource/content/1/b_Swaroop_Byte_of_python.pdf\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   619  100   619    0     0   4918      0 --:--:-- --:--:-- --:--:--  4952\n100   361  100   361    0     0   1684      0 --:--:-- --:--:-- --:--:--  1684\n  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0  0     0    0     0    0     0      0      0 --:--:--  0:00:04 --:--:--     0  0     0    0     0    0     0      0      0 --:--:--  0:00:05 --:--:--     0  2 2070k    2 48897    0     0   8284      0  0:04:15  0:00:05  0:04:10 10846100 2070k  100 2070k    0     0   334k      0  0:00:06  0:00:06 --:--:--  546k\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100 47511  100 47511    0     0  28141      0  0:00:01  0:00:01 --:--:-- 28129100 47511  100 47511    0     0  28138      0  0:00:01  0:00:01 --:--:-- 28129\n\n\nLet’s navigate back to the dataset root (DataLad-101 folder) and run he tree command which can visualize the directory hierarchy:\n\ncd ../\n\n\ntree\n\n.\n├── Dockerfile\n├── LICENSE\n├── Makefile\n├── README.md\n├── _colab_install.md\n├── _extensions\n│   └── mcanouil\n│       └── collapse-output\n│           ├── LICENSE\n│           ├── _extension.yml\n│           ├── _modules\n│           │   └── utils.lua\n│           ├── collapse-output.js\n│           ├── collapse-output.lua\n│           └── collapse-output.min.js\n├── _quarto-ci.yml\n├── _quarto-ipynb.yml\n├── _quarto.yml\n├── _site\n├── _variables.yml\n├── binder\n│   ├── apt.txt\n│   ├── environment.yml\n│   └── postBuild\n├── books\n│   ├── TLCL.pdf\n│   └── byte-of-python.pdf\n├── convert_colab.py\n├── generate_install.py\n├── index.qmd\n├── index.quarto_ipynb\n├── pyproject.toml\n├── setup.qmd\n├── tutorial.qmd\n├── tutorial.quarto_ipynb\n└── uv.lock\n\n8 directories, 29 files\n\n\nUse the datalad status command to find out what has happened in the dataset. This command is very helpful as it reports on the current state of your dataset. Any new or changed content will be highlighted. If nothing has changed, the datalad status command will report what is known as a clean dataset state. In general, it is very useful to maintain a clean dataset state. If you know Git, you can think about datalad status as the git status of DataLad.\n\ndatalad status\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\nCommandError: 'git -c diff.ignoreSubmodules=none -c core.quotepath=false for-each-ref '--format=%(refname:strip=2)' refs/heads refs/remotes' failed with exitcode 128\nfatal: detected dubious ownership in repository at '/app'\nTo add an exception for this directory, call:\n\n    git config --global --add safe.directory /app\n\n\n: 128\n\n\nAny content that we want DataLad to manage needs to be explicitly added to DataLad. It is not enough to simply place it inside the dataset. To give new or changed content to DataLad, we need to save it using datalad save. This is the first time we need to specify a “commit message”, which is done using the -m option of the command. A “commit” is a snapshot of your project’s files at a specific point in time. The commit message is a short text description that explains the changes made when saving the current changes in a Datalad dataset.\n\ndatalad save -m \"Add books on Python and Unix to read later\"\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\nCommandError: 'git -c diff.ignoreSubmodules=none -c core.quotepath=false for-each-ref '--format=%(refname:strip=2)' refs/heads refs/remotes' failed with exitcode 128\nfatal: detected dubious ownership in repository at '/app'\nTo add an exception for this directory, call:\n\n    git config --global --add safe.directory /app\n\n\n: 128\n\n\nWith git log -n 1 you can take a look at the most recent commit in the history:\n\ngit log -n 1\n\nfatal: detected dubious ownership in repository at '/app'\nTo add an exception for this directory, call:\n\n    git config --global --add safe.directory /app\n\n\n: 128\n\n\ndatalad save saves all untracked content to the dataset. Sometimes, this can be inconvenient. One significant advantage of a dataset’s history is that it allows you to revert changes you are not happy with. However, this is only easily possible at the level of single commits. If one save commits several unrelated files or changes, it can be difficult to disentangle them if you ever want to revert some of those changes. To address this, you can provide a path to the specific file you want to save, allowing you to specify more precisely what will be saved together.\nLet’s demonstrate this by adding another book from the internet:\n\ncd books\n\n\ncurl -L -o progit.pdf https://github.com/progit/progit2/releases/download/2.1.154/progit.pdf\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100 11.8M  100 11.8M    0     0  18.3M      0 --:--:-- --:--:-- --:--:-- 18.3M\n\n\n\ncd ../\n\n\ndatalad status\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\nCommandError: 'git -c diff.ignoreSubmodules=none -c core.quotepath=false for-each-ref '--format=%(refname:strip=2)' refs/heads refs/remotes' failed with exitcode 128\nfatal: detected dubious ownership in repository at '/app'\nTo add an exception for this directory, call:\n\n    git config --global --add safe.directory /app\n\n\n: 128\n\n\nNow when you run datalad save, attach a path to the command:\n\ndatalad save -m \"Add reference book about Git\" books/progit.pdf\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\nCommandError: 'git -c diff.ignoreSubmodules=none -c core.quotepath=false for-each-ref '--format=%(refname:strip=2)' refs/heads refs/remotes' failed with exitcode 128\nfatal: detected dubious ownership in repository at '/app'\nTo add an exception for this directory, call:\n\n    git config --global --add safe.directory /app\n\n\n: 128\n\n\nLet’s take a look at files that are frequently modified, such as code or text. To demonstrate this, we will create a file and modify it. We will use a here doc for this, but you can also write the note using an editor of your choice. If you execute this code snippet, make sure to copy and paste everything, starting with cat and ending with the second EOT.\n\ncat &lt;&lt; EOT &gt; notes.txt\nA DataLad dataset can be created with \"datalad create PATH\".\nThe dataset is created empty.\n\nEOT\n\ndatalad status will, as expected, say that there is a new untracked file in the dataset:\n\ndatalad status\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\nCommandError: 'git -c diff.ignoreSubmodules=none -c core.quotepath=false for-each-ref '--format=%(refname:strip=2)' refs/heads refs/remotes' failed with exitcode 128\nfatal: detected dubious ownership in repository at '/app'\nTo add an exception for this directory, call:\n\n    git config --global --add safe.directory /app\n\n\n: 128\n\n\nWe can save the newly created notes.txt-file with the datalad save command and a helpful commit message. As this is the only change in the dataset, there is no need to provide a path:\n\ndatalad save -m \"Add notes on datalad create\"\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\nCommandError: 'git -c diff.ignoreSubmodules=none -c core.quotepath=false for-each-ref '--format=%(refname:strip=2)' refs/heads refs/remotes' failed with exitcode 128\nfatal: detected dubious ownership in repository at '/app'\nTo add an exception for this directory, call:\n\n    git config --global --add safe.directory /app\n\n\n: 128\n\n\nLet’s now add another note to modifiy this file:\n\ncat &lt;&lt; EOT &gt;&gt; notes.txt\nThe command \"datalad save [-m] PATH\" saves the file (modifications) to history.\nNote to self: Always use informative and concise commit messages.\n\nEOT\n\nA datalad status command reports the file as not untracked. However, because it differs from the state it was saved under, it is reported as modified.\n\ndatalad status\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\nCommandError: 'git -c diff.ignoreSubmodules=none -c core.quotepath=false for-each-ref '--format=%(refname:strip=2)' refs/heads refs/remotes' failed with exitcode 128\nfatal: detected dubious ownership in repository at '/app'\nTo add an exception for this directory, call:\n\n    git config --global --add safe.directory /app\n\n\n: 128\n\n\nLet’s save this:\n\ndatalad save -m \"Add notes on datalad save\"\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\nCommandError: 'git -c diff.ignoreSubmodules=none -c core.quotepath=false for-each-ref '--format=%(refname:strip=2)' refs/heads refs/remotes' failed with exitcode 128\nfatal: detected dubious ownership in repository at '/app'\nTo add an exception for this directory, call:\n\n    git config --global --add safe.directory /app\n\n\n: 128\n\n\nIf you take a look at the history of this file with git log, the history neatly summarizes all of the changes that have been done:\n\ngit log -n 2\n\nfatal: detected dubious ownership in repository at '/app'\nTo add an exception for this directory, call:\n\n    git config --global --add safe.directory /app\n\n\n: 128"
  },
  {
    "objectID": "tutorial.html#dataset-consumption",
    "href": "tutorial.html#dataset-consumption",
    "title": "Version Control of Data with DataLad",
    "section": "Dataset consumption",
    "text": "Dataset consumption\nDataLad lets you consume datasets provided by others, and collaborate with them. You can install existing datasets and update them from their sources, or create sibling datasets that you can publish updates to and pull updates from for collaboration and data sharing.\n\nTo demonstrate this, let’s first create a new subdirectory to be organized:\n\nmkdir recordings\n\nAfterwards, let’s install an existing dataset, either from a path or a URL. The dataset we want to install in this example is hosted on GitHub, so we will provide its URL to the datalad clone command. We will also specify a path where we want it to be installed. Importantly, we are installing this dataset as a subdataset of DataLad-101, which means we will nest the two datasets inside each other. This is accomplished using the --dataset flag.\n\ndatalad clone --dataset . https://github.com/datalad-datasets/longnow-podcasts.git recordings/longnow\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\nCommandError: 'git -c diff.ignoreSubmodules=none -c core.quotepath=false for-each-ref '--format=%(refname:strip=2)' refs/heads refs/remotes' failed with exitcode 128\nfatal: detected dubious ownership in repository at '/app'\nTo add an exception for this directory, call:\n\n    git config --global --add safe.directory /app\n\n\n: 128\n\n\nThere are new directories in the DataLad-101 dataset. Within these new directories, there are hundreds of MP3 files.\n\ntree -d # we limit the output of the tree command to directories\n\n.\n├── _extensions\n│   └── mcanouil\n│       └── collapse-output\n│           └── _modules\n├── _site\n├── binder\n├── books\n└── recordings\n\n9 directories\n\n\nLet’s move into one of these directories and take a look at its contents:\n\ncd recordings/longnow/Long_Now__Seminars_About_Long_term_Thinking\n\nbash: cd: recordings/longnow/Long_Now__Seminars_About_Long_term_Thinking: No such file or directory\n\n\n: 1\n\n\n\nls\n\nDockerfile     _quarto.yml      index.quarto_ipynb\nLICENSE        _site        notes.txt\nMakefile       _variables.yml   pyproject.toml\nREADME.md      binder       recordings\n_colab_install.md  books        setup.qmd\n_extensions    convert_colab.py tutorial.qmd\n_quarto-ci.yml     generate_install.py  tutorial.quarto_ipynb\n_quarto-ipynb.yml  index.qmd        uv.lock\n\n\n\nHave access to more data than you have disk space: get and drop\nHere is a crucial and incredibly handy feature of DataLad datasets: After cloning, the dataset contains small files, such as the README, but larger files do not have any content yet. It only retrieved what we can simplistically refer to as file availability metadata, which is displayed as the file hierarchy in the dataset. While we can read the file names and determine what the dataset contains, we don’t have access to the file contents yet. If we were to try to play one of the recordings using the Python Audio functionality, this would fail.\n\n# vlc --intf dummy --play-and-exit \"2003_11_15__Brian_Eno__The_Long_Now.mp3\"\n\nThis might seem like curious behavior, but there are many advantages to it. One advantage is speed, and another is reduced disk usage. Here is the total size of this dataset:\n\ncd ../\n\n\ndu -sh  # Unix command to show size of contents\n\ndu: cannot access './proc/258/task/258/fd/4': No such file or directory\ndu: cannot access './proc/258/task/258/fdinfo/4': No such file or directory\ndu: cannot access './proc/258/fd/3': No such file or directory\ndu: cannot access './proc/258/fdinfo/3': No such file or directory\n2.1G    .\n\n\n: 1\n\n\nIt is tiny! However, we can also find out how large the dataset would be if we had all of its contents by using datalad status with the --annex flag. In total, there are more than 15 GB of podcasts that you now have access to.\n\ndatalad status --annex\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n[ERROR  ] datalad.support.exceptions.NoDatasetFound(No dataset found at '/' for the purpose 'report status'.  Specify a dataset to work with by providing its path via the `dataset` option, or change the current working directory to be in a dataset.) (NoDatasetFound) \n\nusage: datalad status [-h] [-d DATASET] [--annex [{basic|availability|all}]]\n\n                      [--untracked {no|normal|all}] [-r] [-R LEVELS]\n\n                      [-e {no|commit|full}] [-t {raw|eval}] [--version]\n\n                      [PATH ...]\n\n\n\n\n: 2\n\n\nYou can retrieve individual files, groups of files, directories, or entire datasets using the datalad get command. This command fetches the content for you.\n\ndatalad get Long_Now__Seminars_About_Long_term_Thinking/2003_11_15__Brian_Eno__The_Long_Now.mp3\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n[ERROR  ] datalad.support.exceptions.NoDatasetFound(No dataset found at '/' for the purpose \"get content of &lt;&lt;['Long_Now__Se++69 chars++p3']&gt;&gt;\".  Specify a dataset to work with by providing its path via the `dataset` option, or change the current working directory to be in a dataset.) (NoDatasetFound) \n\nusage: datalad get [-h] [-s LABEL] [-d PATH] [-r] [-R LEVELS] [-n]\n\n                   [-D DESCRIPTION] [--reckless [auto|ephemeral|shared-...]]\n\n                   [-J NJOBS] [--version]\n\n                   [PATH ...]\n\n\n\n\n: 2\n\n\nContent that is already present is not re-retrieved.\n\ndatalad get Long_Now__Seminars_About_Long_term_Thinking/2003_11_15__Brian_Eno__The_Long_Now.mp3  \\Long_Now__Seminars_About_Long_term_Thinking/2003_12_13__Peter_Schwartz__The_Art_Of_The_Really_Long_View.mp3  \\Long_Now__Seminars_About_Long_term_Thinking/2004_01_10__George_Dyson__There_s_Plenty_of_Room_at_the_Top__Long_term_Thinking_About_Large_scale_Computing.mp3\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n[ERROR  ] datalad.support.exceptions.NoDatasetFound(No dataset found at '/' for the purpose \"get content of &lt;&lt;['Long_Now__Se++339 chars++p3']&gt;&gt;\".  Specify a dataset to work with by providing its path via the `dataset` option, or change the current working directory to be in a dataset.) (NoDatasetFound) \n\nusage: datalad get [-h] [-s LABEL] [-d PATH] [-r] [-R LEVELS] [-n]\n\n                   [-D DESCRIPTION] [--reckless [auto|ephemeral|shared-...]]\n\n                   [-J NJOBS] [--version]\n\n                   [PATH ...]\n\n\n\n\n: 2\n\n\nIf you no longer need the data locally, you can drop the content from your dataset to save disk space.\n\ndatalad drop Long_Now__Seminars_About_Long_term_Thinking/2003_12_13__Peter_Schwartz__The_Art_Of_The_Really_Long_View.mp3\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n[ERROR  ] datalad.support.exceptions.NoDatasetFound(No dataset found at '/' for the purpose 'drop'.  Specify a dataset to work with by providing its path via the `dataset` option, or change the current working directory to be in a dataset.) (NoDatasetFound) \n\nusage: datalad drop [-h] [--what {filecontent|allkeys|datasets|all}]\n\n                    [--reckless {modification|availability|undead|kill}]\n\n                    [-d DATASET] [-r] [-R LEVELS] [-J NJOBS] [--nocheck]\n\n                    [--if-dirty IF_DIRTY] [--version]\n\n                    [PATH ...]\n\n\n\n\n: 2\n\n\nAfterwards, as long as DataLad knows where a file came from, its content can be retrieved again.\n\ndatalad get Long_Now__Seminars_About_Long_term_Thinking/2003_12_13__Peter_Schwartz__The_Art_Of_The_Really_Long_View.mp3\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n[ERROR  ] datalad.support.exceptions.NoDatasetFound(No dataset found at '/' for the purpose \"get content of &lt;&lt;['Long_Now__Se++93 chars++p3']&gt;&gt;\".  Specify a dataset to work with by providing its path via the `dataset` option, or change the current working directory to be in a dataset.) (NoDatasetFound) \n\nusage: datalad get [-h] [-s LABEL] [-d PATH] [-r] [-R LEVELS] [-n]\n\n                   [-D DESCRIPTION] [--reckless [auto|ephemeral|shared-...]]\n\n                   [-J NJOBS] [--version]\n\n                   [PATH ...]\n\n\n\n\n: 2"
  },
  {
    "objectID": "tutorial.html#dataset-nesting",
    "href": "tutorial.html#dataset-nesting",
    "title": "Version Control of Data with DataLad",
    "section": "Dataset nesting",
    "text": "Dataset nesting\nDatasets can contain other datasets (subdatasets), nested arbitrarily deep. Each dataset has an independent revision history, but can be registered at a precise version in higher-level datasets. This allows to combine datasets and to perform commands recursively across a hierarchy of datasets, and it is the basis for advanced provenance capture abilities.\n\nLet’s take a look at the history of the longnow subdataset. We can see that it has preserved its history completely. This means that the data we retrieved retains all of its provenance.\n\ngit log --reverse\n\nfatal: not a git repository (or any of the parent directories): .git\n\n\n: 128\n\n\nHow does this look in the top-level dataset? If we query the history of DataLad-101, there will be no commits related to MP3 files or any of the commits we have seen in the subdataset. Instead, we can see that the superdataset recorded the recordings/longnow dataset as a subdataset. This means it recorded where this dataset came from and what version it is in.\n\ncd ../../\n\n\ngit log -n 1\n\nfatal: not a git repository (or any of the parent directories): .git\n\n\n: 128\n\n\nThe subproject commit registered the most recent commit of the subdataset, and thus the subdataset version:\n\ncd recordings/longnow\n\nbash: cd: recordings/longnow: No such file or directory\n\n\n: 1\n\n\n\ngit log --oneline\n\nfatal: not a git repository (or any of the parent directories): .git\n\n\n: 128\n\n\n\ncd ../../"
  },
  {
    "objectID": "tutorial.html#more-on-data-versioning-nesting-and-a-glimpse-into-reproducible-paper",
    "href": "tutorial.html#more-on-data-versioning-nesting-and-a-glimpse-into-reproducible-paper",
    "title": "Version Control of Data with DataLad",
    "section": "More on data versioning, nesting, and a glimpse into reproducible paper",
    "text": "More on data versioning, nesting, and a glimpse into reproducible paper\nWe’ll clone a repository for a paper that shares manuscript, code, and data:\n\ncd ../\n\n\ndatalad clone https://github.com/psychoinformatics-de/paper-remodnav.git\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n\nCloning:   0%|                             | 0.00/2.00 [00:00&lt;?, ? candidates/s]\n\n\nEnumerating: 0.00 Objects [00:00, ? Objects/s]\n\n\n                                              \n\n\nCounting:   0%|                                | 0.00/802 [00:00&lt;?, ? Objects/s]\n\n\n                                                                                \n\n\nCompressing:   0%|                             | 0.00/375 [00:00&lt;?, ? Objects/s]\n\n\n                                                                                \n\n\nReceiving:   0%|                             | 0.00/2.11k [00:00&lt;?, ? Objects/s]\n\n\nReceiving:  19%|████▏                 | 401/2.11k [00:00&lt;00:00, 3.08k Objects/s]\n\n\nReceiving:  34%|███████▍              | 718/2.11k [00:00&lt;00:00, 1.44k Objects/s]\n\n\nReceiving:  58%|███████████▌        | 1.22k/2.11k [00:00&lt;00:00, 2.24k Objects/s]\n\n\n                                                                                \n\n\nResolving:   0%|                              | 0.00/1.09k [00:00&lt;?, ? Deltas/s]\n\n\n                                                                                \n                                                                                \ninstall(error): /paper-remodnav (dataset) [No working git-annex installation of version &gt;= 10.20230126. Visit http://handbook.datalad.org/r.html?install for instructions on how to install DataLad and git-annex.] [No working git-annex installation of version &gt;= 10.20230126. Visit http://handbook.datalad.org/r.html?install for instructions on how to install DataLad and git-annex.]\n\n\n\n\n: 1\n\n\nThe top-level dataset has many subdatasets. One of it, remodnav, is a dataset that contains the sourcecode for a Python package called remodnav used in eyetracking analyses:\n\ncd paper-remodnav\n\nbash: cd: paper-remodnav: No such file or directory\n\n\n: 1\n\n\n\ndatalad subdatasets\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n[ERROR  ] datalad.support.exceptions.NoDatasetFound(No dataset found at '/' for the purpose 'report on subdataset(s)'.  Specify a dataset to work with by providing its path via the `dataset` option, or change the current working directory to be in a dataset.) (NoDatasetFound) \n\nusage: datalad subdatasets [-h] [-d DATASET] [--state {present|absent|any}]\n\n                           [--fulfilled FULFILLED] [-r] [-R LEVELS]\n\n                           [--contains PATH] [--bottomup]\n\n                           [--set-property NAME VALUE]\n\n                           [--delete-property NAME] [--version]\n\n                           [PATH ...]\n\n\n\n\n: 2\n\n\nAfter cloning a dataset, its subdatasets will be recognized, but just as content is not yet retrieved for files in datasets, the subdatasets of datasets are not yet installed. If we navigate into an uninstalled subdataset, it will appear as an empty directory.\n\ncd remodnav\n\nbash: cd: remodnav: No such file or directory\n\n\n: 1\n\n\n\nls\n\napp  boot  etc   lib    media  opt   root  sbin  sys  usr\nbin  dev   home  lib64  mnt    proc  run   srv   tmp  var\n\n\nIn order to install a subdataset, we use datalad get with the --recursive flag:\n\ndatalad get --recursive --recursion-limit 2 -n .\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n[ERROR  ] datalad.support.exceptions.NoDatasetFound(No dataset found at '/' for the purpose \"get content of ['.']\".  Specify a dataset to work with by providing its path via the `dataset` option, or change the current working directory to be in a dataset.) (NoDatasetFound) \n\nusage: datalad get [-h] [-s LABEL] [-d PATH] [-r] [-R LEVELS] [-n]\n\n                   [-D DESCRIPTION] [--reckless [auto|ephemeral|shared-...]]\n\n                   [-J NJOBS] [--version]\n\n                   [PATH ...]\n\n\n\n\n: 2\n\n\n\nls\n\napp  boot  etc   lib    media  opt   root  sbin  sys  usr\nbin  dev   home  lib64  mnt    proc  run   srv   tmp  var\n\n\nThis command not only retrieves file contents, but it also installs subdatasets. So, if you want to be really lazy, just run datalad get --recursive -n in the root of a dataset to install all available subdatasets. The -n option prevents get from downloading any data, so only the subdatasets are installed without any data being downloaded. Here, the depth of recursion is limited. For one, it would take a while to install all subdatasets, but the very raw eye tracking dataset contains subject IDs that should not be shared. Therefore, this subdataset is not accessible. If you try to install all subdatasets, the source eye tracking data will throw an error, because it is not made publicly available.\nAfterwards, you can see that the remodnav subdataset also contains further subdatasets. In this case, these subdatasets contain data used for testing and validating software performance.\n\ndatalad subdatasets\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n[ERROR  ] datalad.support.exceptions.NoDatasetFound(No dataset found at '/' for the purpose 'report on subdataset(s)'.  Specify a dataset to work with by providing its path via the `dataset` option, or change the current working directory to be in a dataset.) (NoDatasetFound) \n\nusage: datalad subdatasets [-h] [-d DATASET] [--state {present|absent|any}]\n\n                           [--fulfilled FULFILLED] [-r] [-R LEVELS]\n\n                           [--contains PATH] [--bottomup]\n\n                           [--set-property NAME VALUE]\n\n                           [--delete-property NAME] [--version]\n\n                           [PATH ...]\n\n\n\n\n: 2\n\n\nOne of the validation data subdatasets came from another lab that shared their data. After the researchers were almost finished with their paper, they found another paper that reported a mistake in this data. The mistake was still present in the data they were using. By inspecting the history of this dataset, you can see that at one point, they contributed a fix that changed the data.\n\ncd remodnav/tests/data/anderson_etal\n\nbash: cd: remodnav/tests/data/anderson_etal: No such file or directory\n\n\n: 1\n\n\n\ngit log -n 3\n\nfatal: not a git repository (or any of the parent directories): .git\n\n\n: 128\n\n\nBecause DataLad can link subdatasets to precise versions, it is possible to consciously decide and openly record which version of the data is used. It is also possible to test how much results change by resetting the subdataset to an earlier state or updating the dataset to a more recent version."
  },
  {
    "objectID": "tutorial.html#full-provenance-capture-and-reproducibility",
    "href": "tutorial.html#full-provenance-capture-and-reproducibility",
    "title": "Version Control of Data with DataLad",
    "section": "Full provenance capture and reproducibility",
    "text": "Full provenance capture and reproducibility\nDataLad allows to capture full provenance (i.e., a record that describes entities and processes that were involved in producing or influencing a digital resource): The origin of datasets, the origin of files obtained from web sources, complete machine-readable and automatically reproducible records of how files were created (including software environments). You or your collaborators can thus reobtain or reproducibly recompute content with a single command, and make use of extensive provenance of dataset content (who created it, when, and how?).\n\n\ncd ../../../../../../\n\nFirst, create a new dataset, in this case with the yoda configuration:\n\ndatalad create -c yoda myanalysis\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n[ERROR  ] No working git-annex installation of version &gt;= 10.20230126. Visit http://handbook.datalad.org/r.html?install for instructions on how to install DataLad and git-annex. \n\n\n\n\n: 1\n\n\nThis sets up a helpful structure for my dataset with a code directory and some README files, and applies helpful configurations:\n\ncd myanalysis\n\n\ntree\n\n.\n\n0 directories, 0 files\n\n\nRead more about the YODA principles and the YODA configuration in the section on YODA in the DataLad Handbook.\nNext, install the input data as a subdataset. For this, the DataLad developers created a DataLad dataset with the “iris” data and published it on GitHub. Here, we’re installing it into a directory named input.\n\ndatalad clone -d . https://github.com/datalad-handbook/iris_data.git input/\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n\nCloning:   0%|                             | 0.00/2.00 [00:00&lt;?, ? candidates/s]\n\n\nEnumerating: 0.00 Objects [00:00, ? Objects/s]\n\n\n                                              \n\n\nCounting:   0%|                               | 0.00/25.0 [00:00&lt;?, ? Objects/s]\n\n\n                                                                                \n\n\nCompressing:   0%|                            | 0.00/19.0 [00:00&lt;?, ? Objects/s]\n\n\n                                                                                \n\n\nReceiving:   0%|                              | 0.00/25.0 [00:00&lt;?, ? Objects/s]\n\n\n                                                                                \n\n\nResolving:   0%|                               | 0.00/3.00 [00:00&lt;?, ? Deltas/s]\n\n\n                                                                                \n                                                                                \ninstall(error): input (dataset) [No working git-annex installation of version &gt;= 10.20230126. Visit http://handbook.datalad.org/r.html?install for instructions on how to install DataLad and git-annex.] [No working git-annex installation of version &gt;= 10.20230126. Visit http://handbook.datalad.org/r.html?install for instructions on how to install DataLad and git-annex.]\n\n\n\n\n: 1\n\n\nThe last thing needed is code to run on the data and produce results. For this, here is a k-means classification analysis script written in Python. You can find more details about this analysis in the section on a YODA-compliant data analysis projects.\n\ncat &lt;&lt; EOT &gt; code/script.py\n\nimport pandas as pd\nimport seaborn as sns\nimport datalad.api as dl\nfrom sklearn import model_selection\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report\n\ndata = \"input/iris.csv\"\n\n# make sure that the data are obtained (get will also install linked sub-ds!):\ndl.get(data)\n\n# prepare the data as a pandas dataframe\ndf = pd.read_csv(data)\nattributes = [\"sepal_length\", \"sepal_width\", \"petal_length\",\"petal_width\", \"class\"]\ndf.columns = attributes\n\n# create a pairplot to plot pairwise relationships in the dataset\nplot = sns.pairplot(df, hue='class', palette='muted')\nplot.savefig('pairwise_relationships.png')\n\n# perform a K-nearest-neighbours classification with scikit-learn\n# Step 1: split data in test and training dataset (20:80)\narray = df.values\nX = array[:,0:4]\nY = array[:,4]\ntest_size = 0.20\nseed = 7\nX_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y,\n                                                                    test_size=test_size,\n                                                                    random_state=seed)\n# Step 2: Fit the model and make predictions on the test dataset\nknn = KNeighborsClassifier()\nknn.fit(X_train, Y_train)\npredictions = knn.predict(X_test)\n\n# Step 3: Save the classification report\nreport = classification_report(Y_test, predictions, output_dict=True)\ndf_report = pd.DataFrame(report).transpose().to_csv('prediction_report.csv')\n\nEOT\n\nbash: code/script.py: No such file or directory\n\n\n: 1\n\n\nSo far the script is untracked:\n\ndatalad status\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\nnothing to save, working tree clean\n\n\nLet’s save it with a datalad save command:\n\ndatalad save -m \"Add script for kNN classification and plotting\" code/script.py\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n\n\ndatalad run\nThe challenge that DataLad helps accomplish is running this script in a way that links the script to the results it produces and the data it was computed from. We can do this with the datalad run command. In principle, it is simple. You start with a clean dataset:\n\ndatalad status\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\nnothing to save, working tree clean\n\n\nThen, give the command you would execute with datalad run, in this case python code/script.py. DataLad will take the command, run it, and save all of the changes in the dataset under the commit message specified with the -m option. Thus, it associates the script with the results.\nBut it can be even more helpful. Here, we also specify the input data that the command needs, and DataLad will retrieve the data beforehand. We also specify the output of the command. Specifying the outputs will allow us to rerun the command later and update any outdated results.\n\ndatalad run -m \"Analyze iris data with classification analysis\" \\\n--input \"input/iris.csv\" \\\n--output \"prediction_report.csv\" \\\n--output \"pairwise_relationships.png\" \\\n\"python3 code/script.py\"\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n[INFO   ] Making sure inputs are available (this may take some time) \n\nrun(error): /myanalysis (dataset) [Input did not match existing file: input/iris.csv]\n\n\n\n\n: 1\n\n\nDataLad creates a commit in the dataset history. This commit includes my commit message as a human-readable summary of what was done. It contains the produced output, and it has a machine-readable record that includes information on the input data, the results, and the command that was run to create this result.\n\ngit log -n 1\n\nfatal: your current branch 'master' does not have any commits yet\n\n\n: 128\n\n\n\n\ndatalad rerun\nThis machine-readable record is particularly helpful, because we can now instruct DataLad to rerun this command. This means we don’t have to memorize what we did, and people that we share the dataset with don’t need to ask how this result was produced. They can simply let DataLad tell them.\nThis is accomplished with the datalad rerun command.\nFor this demonstration, we have prepared this analysis dataset and published it to GitHub at https://github.com/lnnrtwttkhn/datalad-tutorial-myanalysis.\n\ncd ../\n\n\ngit clone https://github.com/lnnrtwttkhn/datalad-tutorial-myanalysis analysis_clone\n\nCloning into 'analysis_clone'...\nremote: Enumerating objects: 37, done.        \nremote: Counting objects:   2% (1/37)        remote: Counting objects:   5% (2/37)        remote: Counting objects:   8% (3/37)        remote: Counting objects:  10% (4/37)        remote: Counting objects:  13% (5/37)        remote: Counting objects:  16% (6/37)        remote: Counting objects:  18% (7/37)        remote: Counting objects:  21% (8/37)        remote: Counting objects:  24% (9/37)        remote: Counting objects:  27% (10/37)        remote: Counting objects:  29% (11/37)        remote: Counting objects:  32% (12/37)        remote: Counting objects:  35% (13/37)        remote: Counting objects:  37% (14/37)        remote: Counting objects:  40% (15/37)        remote: Counting objects:  43% (16/37)        remote: Counting objects:  45% (17/37)        remote: Counting objects:  48% (18/37)        remote: Counting objects:  51% (19/37)        remote: Counting objects:  54% (20/37)        remote: Counting objects:  56% (21/37)        remote: Counting objects:  59% (22/37)        remote: Counting objects:  62% (23/37)        remote: Counting objects:  64% (24/37)        remote: Counting objects:  67% (25/37)        remote: Counting objects:  70% (26/37)        remote: Counting objects:  72% (27/37)        remote: Counting objects:  75% (28/37)        remote: Counting objects:  78% (29/37)        remote: Counting objects:  81% (30/37)        remote: Counting objects:  83% (31/37)        remote: Counting objects:  86% (32/37)        remote: Counting objects:  89% (33/37)        remote: Counting objects:  91% (34/37)        remote: Counting objects:  94% (35/37)        remote: Counting objects:  97% (36/37)        remote: Counting objects: 100% (37/37)        remote: Counting objects: 100% (37/37), done.        \nremote: Compressing objects:   4% (1/24)        remote: Compressing objects:   8% (2/24)        remote: Compressing objects:  12% (3/24)        remote: Compressing objects:  16% (4/24)        remote: Compressing objects:  20% (5/24)        remote: Compressing objects:  25% (6/24)        remote: Compressing objects:  29% (7/24)        remote: Compressing objects:  33% (8/24)        remote: Compressing objects:  37% (9/24)        remote: Compressing objects:  41% (10/24)        remote: Compressing objects:  45% (11/24)        remote: Compressing objects:  50% (12/24)        remote: Compressing objects:  54% (13/24)        remote: Compressing objects:  58% (14/24)        remote: Compressing objects:  62% (15/24)        remote: Compressing objects:  66% (16/24)        remote: Compressing objects:  70% (17/24)        remote: Compressing objects:  75% (18/24)        remote: Compressing objects:  79% (19/24)        remote: Compressing objects:  83% (20/24)        remote: Compressing objects:  87% (21/24)        remote: Compressing objects:  91% (22/24)        remote: Compressing objects:  95% (23/24)        remote: Compressing objects: 100% (24/24)        remote: Compressing objects: 100% (24/24), done.        \nremote: Total 37 (delta 6), reused 37 (delta 6), pack-reused 0 (from 0)        \nReceiving objects:   2% (1/37)Receiving objects:   5% (2/37)Receiving objects:   8% (3/37)Receiving objects:  10% (4/37)Receiving objects:  13% (5/37)Receiving objects:  16% (6/37)Receiving objects:  18% (7/37)Receiving objects:  21% (8/37)Receiving objects:  24% (9/37)Receiving objects:  27% (10/37)Receiving objects:  29% (11/37)Receiving objects:  32% (12/37)Receiving objects:  35% (13/37)Receiving objects:  37% (14/37)Receiving objects:  40% (15/37)Receiving objects:  43% (16/37)Receiving objects:  45% (17/37)Receiving objects:  48% (18/37)Receiving objects:  51% (19/37)Receiving objects:  54% (20/37)Receiving objects:  56% (21/37)Receiving objects:  59% (22/37)Receiving objects:  62% (23/37)Receiving objects:  64% (24/37)Receiving objects:  67% (25/37)Receiving objects:  70% (26/37)Receiving objects:  72% (27/37)Receiving objects:  75% (28/37)Receiving objects:  78% (29/37)Receiving objects:  81% (30/37)Receiving objects:  83% (31/37)Receiving objects:  86% (32/37)Receiving objects:  89% (33/37)Receiving objects:  91% (34/37)Receiving objects:  94% (35/37)Receiving objects:  97% (36/37)Receiving objects: 100% (37/37)Receiving objects: 100% (37/37), 4.22 KiB | 4.22 MiB/s, done.\nResolving deltas:   0% (0/6)Resolving deltas:  16% (1/6)Resolving deltas:  33% (2/6)Resolving deltas:  50% (3/6)Resolving deltas:  66% (4/6)Resolving deltas:  83% (5/6)Resolving deltas: 100% (6/6)Resolving deltas: 100% (6/6), done.\n\n\n\ncd analysis_clone\n\nWe can clone this repository and provide, for example, the checksum of the run command to the datalad rerun command. DataLad will read the machine-readable record of what was done and recompute the exact same thing.\n\ndatalad rerun 3bb049d\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n[INFO   ] run commit 3bb049d; (Analyze iris data...) \n\n[INFO   ] Making sure inputs are available (this may take some time) \n\n\nCloning:   0%|                             | 0.00/4.00 [00:00&lt;?, ? candidates/s]\n\n\nEnumerating: 0.00 Objects [00:00, ? Objects/s]\n\n\n                                              \n\n\nCounting:   0%|                               | 0.00/25.0 [00:00&lt;?, ? Objects/s]\n\n\n                                                                                \n\n\nCompressing:   0%|                            | 0.00/19.0 [00:00&lt;?, ? Objects/s]\n\n\n                                                                                \n\n\nReceiving:   0%|                              | 0.00/25.0 [00:00&lt;?, ? Objects/s]\n\n\n                                                                                \n\n\nResolving:   0%|                               | 0.00/3.00 [00:00&lt;?, ? Deltas/s]\n\n\n                                                                                \n                                                                                \ninstall(error): input (dataset) [No working git-annex installation of version &gt;= 10.20230126. Visit http://handbook.datalad.org/r.html?install for instructions on how to install DataLad and git-annex.] [No working git-annex installation of version &gt;= 10.20230126. Visit http://handbook.datalad.org/r.html?install for instructions on how to install DataLad and git-annex.]\n\n[ERROR  ] No working git-annex installation of version &gt;= 10.20230126. Visit http://handbook.datalad.org/r.html?install for instructions on how to install DataLad and git-annex. \n\n\n\n\n: 1\n\n\nThis allows others to easily rerun my computations. It also spares you the need to remember how you executed a script, and you can inquire about where the results came from.\n\ngit log pairwise_relationships.png\n\ncommit 3bb049dfdf42d5fd08e12b064a1eb8423951fad3 (HEAD -&gt; main, origin/main, origin/HEAD)\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Mon Jul 7 21:54:32 2025 +0200\n\n    [DATALAD RUNCMD] Analyze iris data with classification analysis\n    \n    === Do not change lines below ===\n    {\n     \"chain\": [],\n     \"cmd\": \"python3 code/script.py\",\n     \"dsid\": \"a60bb21c-c42a-439a-aca4-9d450d33ae63\",\n     \"exit\": 0,\n     \"extra_inputs\": [],\n     \"inputs\": [\n      \"input/iris.csv\"\n     ],\n     \"outputs\": [\n      \"prediction_report.csv\",\n      \"pairwise_relationships.png\"\n     ],\n     \"pwd\": \".\"\n    }\n    ^^^ Do not change lines above ^^^\n\n\nDone! Thanks for coding along!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home: Research Data Management with DataLad",
    "section": "",
    "text": "NoteAcknowledgements\n\n\n\n\n\nThis tutorial was initially created by Adina Wagner for the 2020 OHBM Brainhack Traintrack session on DataLad. This notebook accompanies this tutorial video by Adina Wagner."
  },
  {
    "objectID": "index.html#what-is-datalad",
    "href": "index.html#what-is-datalad",
    "title": "Home: Research Data Management with DataLad",
    "section": "What is DataLad?",
    "text": "What is DataLad?\nDataLad is a data management multitool that can assist you in handling the entire life cycle of digital objects. It is a command-line tool, free and open source, and available for all major operating systems. In the command line, all operations begin with the general datalad command."
  },
  {
    "objectID": "index.html#tutorial-contents",
    "href": "index.html#tutorial-contents",
    "title": "Home: Research Data Management with DataLad",
    "section": "Tutorial contents",
    "text": "Tutorial contents\nThis tutorial covers:\n\nIntroduction and setup - Getting started with DataLad\nCreating a DataLad dataset - Basic dataset creation and management\nVersion control workflows - Managing data and code changes over time\nDataset consumption and nesting - Working with existing datasets and subdatasets\nDataset nesting - Advanced nested dataset structures\nMore on data versioning, nesting, and a glimpse into reproducible paper - Real-world examples\nFull provenance capture and reproducibility - Complete workflow tracking and replication"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Home: Research Data Management with DataLad",
    "section": "Getting started",
    "text": "Getting started\nTo start the tutorial:\n\nVisit the Setup page for installation instructions\nFollow along with the Tutorial for hands-on practice"
  },
  {
    "objectID": "index.html#key-features-of-datalad",
    "href": "index.html#key-features-of-datalad",
    "title": "Home: Research Data Management with DataLad",
    "section": "Key features of DataLad",
    "text": "Key features of DataLad\n\nVersion control for data - Track changes to datasets of any size\nData consumption - Install and manage datasets from remote sources\nReproducible workflows - Capture complete provenance of data processing\nCollaboration - Share and synchronize datasets across teams\nStorage flexibility - Work with data locally or on remote storage"
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Home: Research Data Management with DataLad",
    "section": "Resources",
    "text": "Resources\n\nDataLad Documentation\nDataLad Handbook\nDataLad on GitHub\nDataLad Installation Guide"
  }
]