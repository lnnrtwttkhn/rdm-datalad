[
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Setup: How to setup and run the tutorial",
    "section": "",
    "text": "Download notebooks from the right sidebar. Select Colab if you are using Google Colab."
  },
  {
    "objectID": "setup.html#download-notebooks",
    "href": "setup.html#download-notebooks",
    "title": "Setup: How to setup and run the tutorial",
    "section": "",
    "text": "Download notebooks from the right sidebar. Select Colab if you are using Google Colab."
  },
  {
    "objectID": "setup.html#computational-environments",
    "href": "setup.html#computational-environments",
    "title": "Setup: How to setup and run the tutorial",
    "section": "Computational environments",
    "text": "Computational environments\n\nJupyterHub at UHH\n\nOpen https://code.min.uni-hamburg.de/\nSelect File &gt; New Launcher\nAt the bottom, select Terminal\nIn the Terminal, enter:\ngit clone https://github.com/lnnrtwttkhn/rdm-datalad\ncd rdm-datalad\nmake install\nFrom the sidebar on the left, you can now open the tutorial notebook in the rdm-datalad folder:\n\ntutorial.ipynb\n\nPaste the installation command into the first cell of the notebook:\nsource .venv/bin/activate\n\n\n\nGoogle Colab\n\nOpen https://colab.research.google.com/\nSign in with a Google account\nSelect File &gt; Open notebook\nSelect Upload and open or drag the notebook file\nPaste the installation command into the first cell of the notebook:\n\n!apt-get install tree\npip install bash-kernel&gt;=0.10.0 datalad&gt;=1.3.1 ipykernel&gt;=7.1.0 nbclient&gt;=0.10.4 nbformat&gt;=5.10.4 pandas&gt;=3.0.0 scikit-learn&gt;=1.8.0 seaborn&gt;=0.13.2\n\n\nTerminal\n\nOpen a Terminal and copy-paste all commands in the tutorial"
  },
  {
    "objectID": "tutorial.html",
    "href": "tutorial.html",
    "title": "Version Control of Data with DataLad",
    "section": "",
    "text": "This tutorial was initially created by Adina Wagner for the 2020 OHBM Brainhack Traintrack session on DataLad. This notebook accompanies this tutorial video by Adina Wagner."
  },
  {
    "objectID": "tutorial.html#acknowledgements",
    "href": "tutorial.html#acknowledgements",
    "title": "Version Control of Data with DataLad",
    "section": "",
    "text": "This tutorial was initially created by Adina Wagner for the 2020 OHBM Brainhack Traintrack session on DataLad. This notebook accompanies this tutorial video by Adina Wagner."
  },
  {
    "objectID": "tutorial.html#introduction-setup",
    "href": "tutorial.html#introduction-setup",
    "title": "Version Control of Data with DataLad",
    "section": "Introduction & setup",
    "text": "Introduction & setup\nDataLad is a data management multitool that assists you in handling the entire lifecycle of digital objects. It is a command-line tool, free and open source, and available for all major operating systems. In the command line, all operations begin with the general datalad command.\n\n# datalad\n\nFor example, I can type datalad --help to find out more about the available commands.\n\nDataLad Python API\nDataLad also has a Python API that can be used in Python scripts and Jupyter notebooks. This is particularly useful for programmatic data management and integration into data analysis workflows.\n\npython3 -c \"import datalad.api as dl; print('DataLad Python API is available')\"\nCode Output\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\nDataLad Python API is available\n\n\n\nIn Python scripts, you can import the DataLad API as follows:\nimport datalad.api as dl\n\n# Example: get data programmatically\n# dl.get('path/to/file')\n\n# Example: save changes programmatically\n# dl.save(message='Automated save from script')\nYou can find more details about how to install DataLad and its dependencies on all operating systems in the DataLad Handbook. This section also explains how to install DataLad on shared machines where you may not have administrative privileges (sudo rights), such as high-performance computing clusters. If you already have DataLad installed, make sure that it is a recent version. You can check the installed version using the datalad --version command:\n\ndatalad --version\nCode Output\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\ndatalad 1.3.1\n\n\n\n\n\nConfiguring your Git identity\nThe first step, if you haven’t done so already, is to configure your Git identity. If you’re new to Git, don’t worry! This configuration simply involves setting your name and email address, which will associate your changes in a project with you as the author.\nBelow, we provide instructions on how to configure your Git identity.\n\n# git config user.name \"Your name\"\n# git config user.email \"Your email address\""
  },
  {
    "objectID": "tutorial.html#creating-a-datalad-dataset",
    "href": "tutorial.html#creating-a-datalad-dataset",
    "title": "Version Control of Data with DataLad",
    "section": "Creating a DataLad dataset",
    "text": "Creating a DataLad dataset\nEvery command in DataLad affects or uses DataLad datasets, the core data structure of DataLad. A dataset is a directory on a computer that DataLad manages. You can create new, empty datasets from scratch and populate them, or transform existing directories into datasets.\n\nLet’s start by creating a new DataLad dataset. Creating a new dataset is accomplished using the datalad create command. This command requires only a name for the dataset. It will then create a new directory with that name and instruct DataLad to manage it.\nAdditionally, the command includes an option, -c text2git. The -c option allows for specific configurations of the dataset at the time of creation. You can find detailed information about the text2git configuration in the DataLad handbook, specifically in the sections on configurations and procedures. Don’t worry about configurations for now. In general, this configuration serves as a very useful standard for datasets.\n\ndatalad create -c text2git DataLad-101\nCode Output\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n[ERROR  ] No working git-annex installation of version &gt;= 10.20230126. Visit http://handbook.datalad.org/r.html?install for instructions on how to install DataLad and git-annex. \n\n\n\n\nCode Output\n\n: 1\n\n\n\nRight after dataset creation, there is a new directory on the computer called DataLad-101. Let’s navigate into this directory using the cd command and list the directory contents using ls.\n\ncd DataLad-101\n\n\nls # ls does not show any output, because the dataset is empty\n\nDatasets have the exciting feature of recording everything done within them. They provide version control for all content managed by DataLad, regardless of its size. Additionally, datasets maintain a complete history that you can interact with. This history is already present, although it is quite short at this point in time. Let’s take a look at it nonetheless. This history exists thanks to Git. You can access the history of a dataset using any tool that displays Git history. For simplicity, we will use Git’s built-in git log command.\n\ngit log\nCode Output\n\nfatal: your current branch 'master' does not have any commits yet\n\n\nCode Output\n\n: 128"
  },
  {
    "objectID": "tutorial.html#version-control-workflows",
    "href": "tutorial.html#version-control-workflows",
    "title": "Version Control of Data with DataLad",
    "section": "Version control workflows",
    "text": "Version control workflows\nBuilding on top of Git and git-annex, DataLad allows you to version control arbitrarily large files in datasets. You can keep track of revisions of data of any size, and view, interact with, or restore any version of your dataset’s history.\n\nLet’s start by creating a books directory using the mkdir command. Next, we will download two books from the internet. Here, we are using the command line tool curl to accomplish this, allowing us to perform all actions from the command line. However, if you prefer, you can also download the books manually and save them into the dataset using a file manager. Remember, a dataset is simply a directory on your computer.\n\nmkdir books\n\n\ncd books\n\n\ncurl -L -o TLCL.pdf https://sourceforge.net/projects/linuxcommand/files/TLCL/19.01/TLCL-19.01.pdf/download\ncurl -L -o byte-of-python.pdf https://edisciplinas.usp.br/pluginfile.php/3252353/mod_resource/content/1/b_Swaroop_Byte_of_python.pdf\nCode Output\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   619  100   619    0     0   3003      0 --:--:-- --:--:-- --:--:--  3004\n100   361  100   361    0     0    991      0 --:--:-- --:--:-- --:--:--   991\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100 2070k  100 2070k    0     0  2021k      0  0:00:01  0:00:01 --:--:-- 3151k\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0 77 47511   77 36816    0     0  26527      0  0:00:01  0:00:01 --:--:-- 26524100 47511  100 47511    0     0  30840      0  0:00:01  0:00:01 --:--:-- 30831\n\n\n\nLet’s navigate back to the dataset root (DataLad-101 folder) and run the tree command which can visualize the directory hierarchy:\n\ncd ../\n\n\ntree\nCode Output\n\n.\n└── books\n    ├── TLCL.pdf\n    └── byte-of-python.pdf\n\n2 directories, 2 files\n\n\n\nUse the datalad status command to find out what has happened in the dataset. This command is very helpful as it reports on the current state of your dataset. Any new or changed content will be highlighted. If nothing has changed, the datalad status command will report what is known as a clean dataset state. In general, it is very useful to maintain a clean dataset state. If you know Git, you can think of datalad status as the git status of DataLad.\n\ndatalad status\nCode Output\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\nuntracked: books (directory)\n\n\n\n\n\nAny content that we want DataLad to manage needs to be explicitly added to DataLad. It is not enough to simply place it inside the dataset. To give new or changed content to DataLad, we need to save it using datalad save. This is the first time we need to specify a “commit message”, which is done using the -m option of the command. A “commit” is a snapshot of your project’s files at a specific point in time. The commit message is a short text description that explains the changes made when saving the current changes in a DataLad dataset.\n\ndatalad save -m \"Add books on Python and Unix to read later\"\nCode Output\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n\nTotal: 0.00 datasets [00:00, ? datasets/s]\nTotal:   0%|                                 | 0.00/1.00 [00:00&lt;?, ? datasets/s]\nTotal:   0%|                                 | 0.00/1.00 [00:00&lt;?, ? datasets/s]\n                                                                                \nadd(ok): books/TLCL.pdf (file)\n\n\nTotal:   0%|                                 | 0.00/1.00 [00:00&lt;?, ? datasets/s]\n                                                                                \n[WARNING] Received an exception datalad.runner.exception.CommandError(CommandError: 'git -c diff.ignoreSubmodules=none -c core.quotepath=false commit -m 'Add books on Python and Unix to read later'' failed with exitcode 128 [err: 'Author identity unknown\n\n\n\n*** Please tell me who you are.\n\n\n\nRun\n\n\n\n  git config --global user.email \"you@example.com\"\n\n  git config --global user.name \"Your Name\"\n\n\n\nto set your account's default identity.\n\nOmit --global to set the identity only in this repository.\n\n\n\nfatal: unable to auto-detect email address (got 'root@e476308d212e.(none)')']). Canceling not-yet running jobs and waiting for completion of running. You can force earlier forceful exit by Ctrl-C. \n\n\nTotal:   0%|                                 | 0.00/1.00 [00:00&lt;?, ? datasets/s]\n                                                                                \n[INFO   ] Canceled 0 out of 0 jobs. 0 left running. \n\n\nTotal:   0%|                                 | 0.00/1.00 [00:00&lt;?, ? datasets/s]\n                                                                                \nadd(ok): books/byte-of-python.pdf (file)\n\n\nTotal:   0%|                                 | 0.00/1.00 [00:00&lt;?, ? datasets/s]CommandError: 'git -c diff.ignoreSubmodules=none -c core.quotepath=false commit -m 'Add books on Python and Unix to read later'' failed with exitcode 128\n\nAuthor identity unknown\n\n\n\n*** Please tell me who you are.\n\n\n\nRun\n\n\n\n  git config --global user.email \"you@example.com\"\n\n  git config --global user.name \"Your Name\"\n\n\n\nto set your account's default identity.\n\nOmit --global to set the identity only in this repository.\n\n\n\nfatal: unable to auto-detect email address (got 'root@e476308d212e.(none)')\n\n\n\n\nCode Output\n\n: 128\n\n\n\nWith git log -n 1 you can take a look at the most recent commit in the history:\n\ngit log -n 1\nCode Output\n\nfatal: your current branch 'master' does not have any commits yet\n\n\nCode Output\n\n: 128\n\n\n\ndatalad save saves all untracked content to the dataset. Sometimes, this can be inconvenient. One significant advantage of a dataset’s history is that it allows you to revert changes you are not happy with. However, this is only easily possible at the level of single commits. If one save commits several unrelated files or changes, it can be difficult to disentangle them if you ever want to revert some of those changes. To address this, you can provide a path to the specific file you want to save, allowing you to specify more precisely what will be saved together.\nLet’s demonstrate this by adding another book from the internet:\n\ncd books\n\n\ncurl -L -o progit.pdf https://github.com/progit/progit2/releases/download/2.1.154/progit.pdf\nCode Output\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n 76 11.8M   76 9366k    0     0  19.6M      0 --:--:-- --:--:-- --:--:-- 19.6M100 11.8M  100 11.8M    0     0  23.6M      0 --:--:-- --:--:-- --:--:-- 74.0M\n\n\n\n\ncd ../\n\n\ndatalad status\nCode Output\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\nuntracked: books/progit.pdf (file)\n\n    added: books/TLCL.pdf (file)\n\n    added: books/byte-of-python.pdf (file)\n\n\n\n\n\nNow when you run datalad save, attach a path to the command:\n\ndatalad save -m \"Add reference book about Git\" books/progit.pdf\nCode Output\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n\nTotal: 0.00 datasets [00:00, ? datasets/s]\nTotal:   0%|                                 | 0.00/1.00 [00:00&lt;?, ? datasets/s]\nTotal:   0%|                                 | 0.00/1.00 [00:00&lt;?, ? datasets/s]\n                                                                                \nadd(ok): books/progit.pdf (file)\n\n\nTotal:   0%|                                 | 0.00/1.00 [00:00&lt;?, ? datasets/s]\n                                                                                \n[WARNING] Received an exception datalad.runner.exception.CommandError(CommandError: 'git -c diff.ignoreSubmodules=none -c core.quotepath=false commit -m 'Add reference book about Git' -- books/progit.pdf' failed with exitcode 128 [err: 'Author identity unknown\n\n\n\n*** Please tell me who you are.\n\n\n\nRun\n\n\n\n  git config --global user.email \"you@example.com\"\n\n  git config --global user.name \"Your Name\"\n\n\n\nto set your account's default identity.\n\nOmit --global to set the identity only in this repository.\n\n\n\nfatal: unable to auto-detect email address (got 'root@e476308d212e.(none)')']). Canceling not-yet running jobs and waiting for completion of running. You can force earlier forceful exit by Ctrl-C. \n\n\nTotal:   0%|                                 | 0.00/1.00 [00:00&lt;?, ? datasets/s]\n                                                                                \n[INFO   ] Canceled 0 out of 0 jobs. 0 left running. \n\n\nTotal:   0%|                                 | 0.00/1.00 [00:00&lt;?, ? datasets/s]CommandError: 'git -c diff.ignoreSubmodules=none -c core.quotepath=false commit -m 'Add reference book about Git' -- books/progit.pdf' failed with exitcode 128\n\nAuthor identity unknown\n\n\n\n*** Please tell me who you are.\n\n\n\nRun\n\n\n\n  git config --global user.email \"you@example.com\"\n\n  git config --global user.name \"Your Name\"\n\n\n\nto set your account's default identity.\n\nOmit --global to set the identity only in this repository.\n\n\n\nfatal: unable to auto-detect email address (got 'root@e476308d212e.(none)')\n\n\n\n\nCode Output\n\n: 128\n\n\n\nLet’s take a look at files that are frequently modified, such as code or text. To demonstrate this, we will create a file and modify it. We will use a here doc for this, but you can also write the note using an editor of your choice. If you execute this code snippet, make sure to copy and paste everything, starting with cat and ending with the second EOT.\n\ncat &lt;&lt; EOT &gt; notes.txt\nA DataLad dataset can be created with \"datalad create PATH\".\nThe dataset is created empty.\n\nEOT\n\ndatalad status will, as expected, say that there is a new untracked file in the dataset:\n\ndatalad status\nCode Output\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\nuntracked: notes.txt (file)\n\n    added: books/TLCL.pdf (file)\n\n    added: books/byte-of-python.pdf (file)\n\n    added: books/progit.pdf (file)\n\n\n\n\n\nWe can save the newly created notes.txt file with the datalad save command and a helpful commit message. As this is the only change in the dataset, there is no need to provide a path:\n\ndatalad save -m \"Add notes on datalad create\"\nCode Output\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n\nTotal: 0.00 datasets [00:00, ? datasets/s]\nTotal:   0%|                                 | 0.00/1.00 [00:00&lt;?, ? datasets/s]\nTotal:   0%|                                 | 0.00/1.00 [00:00&lt;?, ? datasets/s]\n                                                                                \nadd(ok): notes.txt (file)\n\n\nTotal:   0%|                                 | 0.00/1.00 [00:00&lt;?, ? datasets/s]\n                                                                                \n[WARNING] Received an exception datalad.runner.exception.CommandError(CommandError: 'git -c diff.ignoreSubmodules=none -c core.quotepath=false commit -m 'Add notes on datalad create' -- books/TLCL.pdf books/byte-of-python.pdf books/progit.pdf notes.txt' failed with exitcode 128 [err: 'Author identity unknown\n\n\n\n*** Please tell me who you are.\n\n\n\nRun\n\n\n\n  git config --global user.email \"you@example.com\"\n\n  git config --global user.name \"Your Name\"\n\n\n\nto set your account's default identity.\n\nOmit --global to set the identity only in this repository.\n\n\n\nfatal: unable to auto-detect email address (got 'root@e476308d212e.(none)')']). Canceling not-yet running jobs and waiting for completion of running. You can force earlier forceful exit by Ctrl-C. \n\n\nTotal:   0%|                                 | 0.00/1.00 [00:00&lt;?, ? datasets/s]\n                                                                                \n[INFO   ] Canceled 0 out of 0 jobs. 0 left running. \n\n\nTotal:   0%|                                 | 0.00/1.00 [00:00&lt;?, ? datasets/s]CommandError: 'git -c diff.ignoreSubmodules=none -c core.quotepath=false commit -m 'Add notes on datalad create' -- books/TLCL.pdf books/byte-of-python.pdf books/progit.pdf notes.txt' failed with exitcode 128\n\nAuthor identity unknown\n\n\n\n*** Please tell me who you are.\n\n\n\nRun\n\n\n\n  git config --global user.email \"you@example.com\"\n\n  git config --global user.name \"Your Name\"\n\n\n\nto set your account's default identity.\n\nOmit --global to set the identity only in this repository.\n\n\n\nfatal: unable to auto-detect email address (got 'root@e476308d212e.(none)')\n\n\n\n\nCode Output\n\n: 128\n\n\n\nLet’s now add another note to modify this file:\n\ncat &lt;&lt; EOT &gt;&gt; notes.txt\nThe command \"datalad save [-m] PATH\" saves the file (modifications) to history.\nNote to self: Always use informative and concise commit messages.\n\nEOT\n\nA datalad status command reports the file as not untracked. However, because it differs from the state it was saved under, it is reported as modified.\n\ndatalad status\nCode Output\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n    added: books/TLCL.pdf (file)\n\n    added: books/byte-of-python.pdf (file)\n\n    added: books/progit.pdf (file)\n\n    added: notes.txt (file)\n\n\n\n\n\nLet’s save this:\n\ndatalad save -m \"Add notes on datalad save\"\nCode Output\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n\nTotal: 0.00 datasets [00:00, ? datasets/s]\n                                          \n[WARNING] Received an exception datalad.runner.exception.CommandError(CommandError: 'git -c diff.ignoreSubmodules=none -c core.quotepath=false commit -m 'Add notes on datalad save' -- books/TLCL.pdf books/byte-of-python.pdf books/progit.pdf notes.txt' failed with exitcode 128 [err: 'Author identity unknown\n\n\n\n*** Please tell me who you are.\n\n\n\nRun\n\n\n\n  git config --global user.email \"you@example.com\"\n\n  git config --global user.name \"Your Name\"\n\n\n\nto set your account's default identity.\n\nOmit --global to set the identity only in this repository.\n\n\n\nfatal: unable to auto-detect email address (got 'root@e476308d212e.(none)')']). Canceling not-yet running jobs and waiting for completion of running. You can force earlier forceful exit by Ctrl-C. \n\n\nTotal: 0.00 datasets [00:00, ? datasets/s]\n                                          \n[INFO   ] Canceled 0 out of 0 jobs. 0 left running. \n\n\nTotal: 0.00 datasets [00:00, ? datasets/s]CommandError: 'git -c diff.ignoreSubmodules=none -c core.quotepath=false commit -m 'Add notes on datalad save' -- books/TLCL.pdf books/byte-of-python.pdf books/progit.pdf notes.txt' failed with exitcode 128\n\nAuthor identity unknown\n\n\n\n*** Please tell me who you are.\n\n\n\nRun\n\n\n\n  git config --global user.email \"you@example.com\"\n\n  git config --global user.name \"Your Name\"\n\n\n\nto set your account's default identity.\n\nOmit --global to set the identity only in this repository.\n\n\n\nfatal: unable to auto-detect email address (got 'root@e476308d212e.(none)')\n\n\n\n\nCode Output\n\n: 128\n\n\n\nIf you take a look at the history of this file with git log, the history neatly summarizes all of the changes that have been done:\n\ngit log -n 2\nCode Output\n\nfatal: your current branch 'master' does not have any commits yet\n\n\nCode Output\n\n: 128"
  },
  {
    "objectID": "tutorial.html#dataset-consumption",
    "href": "tutorial.html#dataset-consumption",
    "title": "Version Control of Data with DataLad",
    "section": "Dataset consumption",
    "text": "Dataset consumption\nDataLad lets you consume datasets provided by others, and collaborate with them. You can install existing datasets and update them from their sources, or create sibling datasets that you can publish updates to and pull updates from for collaboration and data sharing.\n\nTo demonstrate this, let’s first create a new subdirectory to be organized:\n\nmkdir recordings\n\nAfterwards, let’s install an existing dataset, either from a path or a URL. The dataset we want to install in this example is hosted on GitHub, so we will provide its URL to the datalad clone command. We will also specify a path where we want it to be installed. Importantly, we are installing this dataset as a subdataset of DataLad-101, which means we will nest the two datasets inside each other. This is accomplished using the --dataset flag.\n\ndatalad clone --dataset . https://github.com/datalad-datasets/longnow-podcasts.git recordings/longnow\nCode Output\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n\nCloning:   0%|                             | 0.00/2.00 [00:00&lt;?, ? candidates/s]\n\n\nEnumerating: 0.00 Objects [00:00, ? Objects/s]\n\n\n                                              \n\n\nReceiving:   0%|                             | 0.00/4.31k [00:00&lt;?, ? Objects/s]\n\n\nReceiving:  11%|██▍                   | 474/4.31k [00:00&lt;00:00, 4.72k Objects/s]\n\n\nReceiving:  51%|██████████▏         | 2.20k/4.31k [00:00&lt;00:00, 12.1k Objects/s]\n\n\n                                                                                \n\n\nResolving:   0%|                                | 0.00/602 [00:00&lt;?, ? Deltas/s]\n\n\n                                                                                \n                                                                                \ninstall(error): recordings/longnow (dataset) [No working git-annex installation of version &gt;= 10.20230126. Visit http://handbook.datalad.org/r.html?install for instructions on how to install DataLad and git-annex.] [No working git-annex installation of version &gt;= 10.20230126. Visit http://handbook.datalad.org/r.html?install for instructions on how to install DataLad and git-annex.]\n\n\n\n\nCode Output\n\n: 1\n\n\n\nThere are new directories in the DataLad-101 dataset. Within these new directories, there are hundreds of MP3 files.\n\ntree -d # we limit the output of the tree command to directories\nCode Output\n\n.\n├── books\n└── recordings\n\n3 directories\n\n\n\nLet’s move into one of these directories and take a look at its contents:\n\ncd recordings/longnow/Long_Now__Seminars_About_Long_term_Thinking\nCode Output\n\nbash: cd: recordings/longnow/Long_Now__Seminars_About_Long_term_Thinking: No such file or directory\n\n\nCode Output\n\n: 1\n\n\n\n\nls\nCode Output\n\nbooks  notes.txt  recordings\n\n\n\n\nHave access to more data than you have disk space: get and drop\nHere is a crucial and incredibly handy feature of DataLad datasets: After cloning, the dataset contains small files, such as the README, but larger files do not have any content yet. It only retrieved what we can simplistically refer to as file availability metadata, which is displayed as the file hierarchy in the dataset. While we can read the file names and determine what the dataset contains, we don’t have access to the file contents yet. If we were to try to play one of the recordings using the Python Audio functionality, this would fail.\n\n# vlc --intf dummy --play-and-exit \"2003_11_15__Brian_Eno__The_Long_Now.mp3\"\n\nThis might seem like curious behavior, but there are many advantages to it. One advantage is speed, and another is reduced disk usage. Here is the total size of this dataset:\n\ncd ../\n\n\ndu -sh  # Unix command to show size of contents\nCode Output\n\n478M    .\n\n\n\nIt is tiny! However, we can also find out how large the dataset would be if we had all of its contents by using datalad status with the --annex flag. In total, there are more than 15 GB of podcasts that you now have access to.\n\ndatalad status --annex\nCode Output\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\nuntracked: DataLad-101 (directory)\n\nuntracked: _extensions (directory)\n\n modified: .gitignore (file)\n\n\n\n\n\nYou can retrieve individual files, groups of files, directories, or entire datasets using the datalad get command. This command fetches the content for you.\n\ndatalad get Long_Now__Seminars_About_Long_term_Thinking/2003_11_15__Brian_Eno__The_Long_Now.mp3\nCode Output\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\nget(impossible): Long_Now__Seminars_About_Long_term_Thinking/2003_11_15__Brian_Eno__The_Long_Now.mp3 [path does not exist]\n\n\n\n\nCode Output\n\n: 1\n\n\n\nContent that is already present is not re-retrieved.\n\ndatalad get Long_Now__Seminars_About_Long_term_Thinking/2003_11_15__Brian_Eno__The_Long_Now.mp3  \\Long_Now__Seminars_About_Long_term_Thinking/2003_12_13__Peter_Schwartz__The_Art_Of_The_Really_Long_View.mp3  \\Long_Now__Seminars_About_Long_term_Thinking/2004_01_10__George_Dyson__There_s_Plenty_of_Room_at_the_Top__Long_term_Thinking_About_Large_scale_Computing.mp3\nCode Output\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\nget(impossible): Long_Now__Seminars_About_Long_term_Thinking/2003_11_15__Brian_Eno__The_Long_Now.mp3 [path does not exist]\n\nget(impossible): Long_Now__Seminars_About_Long_term_Thinking/2003_12_13__Peter_Schwartz__The_Art_Of_The_Really_Long_View.mp3 [path does not exist]\n\nget(impossible): Long_Now__Seminars_About_Long_term_Thinking/2004_01_10__George_Dyson__There_s_Plenty_of_Room_at_the_Top__Long_term_Thinking_About_Large_scale_Computing.mp3 [path does not exist]\n\naction summary:\n\n  get (impossible: 3)\n\n\n\n\nCode Output\n\n: 1\n\n\n\nIf you no longer need the data locally, you can drop the content from your dataset to save disk space.\n\ndatalad drop Long_Now__Seminars_About_Long_term_Thinking/2003_12_13__Peter_Schwartz__The_Art_Of_The_Really_Long_View.mp3\nCode Output\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n\n\nAfterwards, as long as DataLad knows where a file came from, its content can be retrieved again.\n\ndatalad get Long_Now__Seminars_About_Long_term_Thinking/2003_12_13__Peter_Schwartz__The_Art_Of_The_Really_Long_View.mp3\nCode Output\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\nget(impossible): Long_Now__Seminars_About_Long_term_Thinking/2003_12_13__Peter_Schwartz__The_Art_Of_The_Really_Long_View.mp3 [path does not exist]\n\n\n\n\nCode Output\n\n: 1"
  },
  {
    "objectID": "tutorial.html#dataset-nesting",
    "href": "tutorial.html#dataset-nesting",
    "title": "Version Control of Data with DataLad",
    "section": "Dataset nesting",
    "text": "Dataset nesting\nDatasets can contain other datasets (subdatasets), nested arbitrarily deep. Each dataset has an independent revision history, but can be registered at a precise version in higher-level datasets. This allows you to combine datasets and to perform commands recursively across a hierarchy of datasets, and it is the basis for advanced provenance capture abilities. This allows you to combine datasets and perform commands recursively across a hierarchy of datasets, and it is the basis for advanced provenance capture abilities.\n\nLet’s take a look at the history of the longnow subdataset. We can see that it has preserved its history completely. This means that the data we retrieved retains all of its provenance.\n\ngit log --reverse\nCode Output\n\ncommit 0d0bfc4a73979cde8d04a0da358f83fb4102881f\nAuthor: Stephan Heunis &lt;jsheunis@gmail.com&gt;\nDate:   Wed Mar 10 09:46:25 2021 +0100\n\n    Initial commit\n\ncommit 4e90ac4c5f1ceb2f8f9aee798edc149e9e7b3f24\nAuthor: Stijn Denissen &lt;sdenissen@Thijss-MacBook-Air.local&gt;\nDate:   Wed Mar 10 18:50:01 2021 +0100\n\n    Add code to download OpenNeuro data\n\ncommit a090c62efc38e4e5d196fb640b92c57229e386ed\nAuthor: Stijn Denissen &lt;sdenissen@Thijss-MacBook-Air.local&gt;\nDate:   Wed Mar 10 18:51:33 2021 +0100\n\n    Add first instructional blocks of jupyter notebook\n\ncommit ca062fe9be0f556ef284afb454fbec93d25f2f7a\nAuthor: Stijn Denissen &lt;sdenissen@Thijss-MacBook-Air.local&gt;\nDate:   Wed Mar 10 19:32:37 2021 +0100\n\n    Download other data, since previous didnt download\n\ncommit a0af2ce3aa8c60298d0f4c43089a3ff71bfa6066\nAuthor: Stijn Denissen &lt;sdenissen@Thijss-MacBook-Air.local&gt;\nDate:   Wed Mar 10 19:32:52 2021 +0100\n\n    Update .gitignore\n\ncommit 4192338f935b0ba4f682ec030b35ce8231d43a44\nAuthor: Stijn Denissen &lt;sdenissen@Thijss-MacBook-Air.local&gt;\nDate:   Wed Mar 10 19:48:59 2021 +0100\n\n    Add binder link to README\n\ncommit 2191b98e79f203d38950f4c0fdfca451c96ecafd\nAuthor: Stijn Denissen &lt;sdenissen@Thijss-MacBook-Air.local&gt;\nDate:   Fri Mar 12 22:05:01 2021 +0100\n\n    Neuro-imaging data visualisation + start of dataframe visualisation\n\ncommit 31d5fcea83b506480e4297a13f9de0c073bc88d3\nAuthor: Stijn Denissen &lt;sdenissen@Thijss-MacBook-Air.local&gt;\nDate:   Fri Mar 12 22:06:36 2021 +0100\n\n    Also download data and project description to include in notebook\n\ncommit 82c63fb212f02639b5215ad310ad0e02123fb329\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Sat Mar 13 12:21:02 2021 +0100\n\n    move code and data into specific subfolders\n\ncommit f5039999e8ac2759bd662c6e6103bd4a3c9a073e\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Sat Mar 13 12:22:13 2021 +0100\n\n    update requirements\n    \n    - set nilearn version\n    - ad nb_black\n\ncommit ef327359f583200bcba6306942a3c4fbb2eaaa82\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Sat Mar 13 12:23:15 2021 +0100\n\n    add set up instructions to work locally on project\n    \n    - includes a set up environment for conda\n\ncommit be8e082971744fd63a61155089c42bf4f26d8fc3\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Sat Mar 13 12:23:30 2021 +0100\n\n    clean README\n\ncommit e6a737ae98046d6ff859904db39268edce464f8d\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Sat Mar 13 12:24:32 2021 +0100\n\n    clean python jupyter book\n    \n    - lint with black\n    - remove output\n\ncommit 117b63a08aecef1c079fa3f080f4f48d1020c929\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Sat Mar 13 14:40:15 2021 +0100\n\n    set up octave visualisation notebook\n    \n    - also use consistent variable names across notebooks\n\ncommit b0f7fd374f4fa0626672d0e869302d658bd62551\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Sat Mar 13 15:41:00 2021 +0100\n\n    update notebooks to use pybids\n\ncommit cf275512f9e5813ce21a6b2a20ecc22537122306\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Sat Mar 13 15:49:48 2021 +0100\n\n    update binder setup files\n\ncommit 0a57ee65b74b26c608fef02a1eea505f3903d9cf\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Sat Mar 13 16:28:03 2021 +0100\n\n    update setup to install git annex\n\ncommit 6b29c89709c9378bc7ca29f9406584a9ff8d57ec\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Sat Mar 13 16:38:25 2021 +0100\n\n    install git annex via apt-get\n\ncommit f052a9652bfa448cdb1e0c2ff6024361e420988d\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Sat Mar 13 16:47:46 2021 +0100\n\n    remove datalad installer and install git annex via conda\n\ncommit e78d82a85d87f5f0df2227302b1bd65a4e1b3d85\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Sat Mar 13 17:08:23 2021 +0100\n\n    get data with datalad instead of aws\n\ncommit 8446090ac817fd263162e5ba070eed58dc96d36f\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Sat Mar 13 17:20:27 2021 +0100\n\n    fix data path\n\ncommit 3b29890c8d9dfa80c2389f50133d6c8251e0ee47\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Sat Mar 13 17:23:36 2021 +0100\n\n    add plot hrf python notebook\n\ncommit fa76cf2deace554b941cca0b657beab185637f0d\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Sat Mar 13 17:51:44 2021 +0100\n\n    fix typo in visulization notebook\n\ncommit c3a8872050d82b102cbd26365586aea4cd59ca01\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Sat Mar 13 19:04:49 2021 +0100\n\n    add  notebook octave HRF\n\ncommit c941717d3307cf4c4ad805b81a1f369b1854ffb7\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Sat Mar 13 19:21:48 2021 +0100\n\n    update visualization tutorials\n\ncommit 782b411eceec7fc10de299bc0b2aaa1d3362b222\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Sun Mar 14 08:36:01 2021 +0100\n\n    add comments in postBuild\n\ncommit 27583b89d86e96ba219db298a594af400872d342\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Sun Mar 14 15:00:10 2021 +0100\n\n    update HRF and design matrix notebooks\n\ncommit c6d497fa48826ca63c65a4ca53e4c935ed68cb6a\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Sun Mar 14 15:00:25 2021 +0100\n\n    add comments to postBuild\n\ncommit 469753ebd2cb673be82d40eb1a8b9f8eea046549\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Sun Mar 14 15:01:03 2021 +0100\n\n    draft notebooks for GLM and octave design matrix\n\ncommit c1713b2b8257c01edfa8b051056d1c6c83d8b33c\nMerge: 31d5fce 469753e\nAuthor: Stijn Denissen &lt;40318470+Sdniss@users.noreply.github.com&gt;\nDate:   Sun Mar 14 16:39:02 2021 +0100\n\n    Merge pull request #1 from Remi-Gau/remi-nilearn_glm\n    \n    [ENH] improve octave and python tutorials\n\ncommit 1fa1943113a4bd46b29fec3201ac2e2d76e33241\nAuthor: Stijn Denissen &lt;sdenissen@Thijss-MacBook-Air.local&gt;\nDate:   Sun Mar 14 16:55:22 2021 +0100\n\n    Rename python notebook\n\ncommit bf6c2a8800bd40f9153dde80ff00a995bde495d8\nMerge: c1713b2 1fa1943\nAuthor: Stijn Denissen &lt;40318470+Sdniss@users.noreply.github.com&gt;\nDate:   Sun Mar 14 16:58:01 2021 +0100\n\n    Merge pull request #2 from Sdniss/development\n    \n    Rename python notebook\n\ncommit 8d8178c74358784353be58097d7c99bd42373c00\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Sun Mar 14 17:21:52 2021 +0100\n\n    move and rename files\n\ncommit 4d1233f71d3b3dd42167da5b9c23eba50ba249e2\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Sun Mar 14 18:25:27 2021 +0100\n\n    octave notebook: use spm to batch to unzip anat and func\n\ncommit 47425fa8ef6dcb04b147bb086c61023428c03eb7\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Sun Mar 14 21:59:18 2021 +0100\n\n    add octave sub-functions to facilitate plotting\n\ncommit da2622041698b8eb6f6ed8ad187fbf244367c942\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Mon Mar 15 07:52:42 2021 +0100\n\n    octave viz notebook : clear ouput\n\ncommit cb3d7240be35590aa1f5b6a71497e236cc5d2982\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Mon Mar 15 08:49:59 2021 +0100\n\n    add show_slice\n\ncommit 38d78fbb5728fad69eac7af87577703b66240ab9\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Mon Mar 15 08:53:15 2021 +0100\n\n    use show_slice\n\ncommit 38f55d7a0c55c5b3e662232fa4b6774fb9afbe85\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Mon Mar 15 08:56:59 2021 +0100\n\n    change case Slice variable\n\ncommit 344c72ff28167811bb8e1e3d1499c1cf5cc11ce4\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Mon Mar 15 08:57:15 2021 +0100\n\n    apply Black Linting\n\ncommit 5c0bdf2fd7bdd36c425e1e91d0a47e37230b96d2\nMerge: bf6c2a8 344c72f\nAuthor: Stijn Denissen &lt;40318470+Sdniss@users.noreply.github.com&gt;\nDate:   Thu Mar 18 13:29:13 2021 +0100\n\n    Merge pull request #7 from Remi-Gau/remi-refactor_python\n    \n    [REF] simplify plotting functions\n\ncommit 6e0080ba44735c550dbd9a7ab481df6fc070708d\nAuthor: Stijn Denissen &lt;stijn.denissen@vub.be&gt;\nDate:   Sat Mar 20 01:49:19 2021 +0100\n\n    Add data simulation function + adapt show_slice\n\ncommit fbf14cbcad0e46e124b53e3a2360285a4bfa5d73\nAuthor: Stijn Denissen &lt;stijn.denissen@vub.be&gt;\nDate:   Sat Mar 20 01:49:43 2021 +0100\n\n    First writing on dataframe visualisation\n\ncommit 52f3bd7db3d1b7b33924a8522990bada95d3d3f8\nAuthor: Stijn Denissen &lt;stijn.denissen@vub.be&gt;\nDate:   Sun Mar 21 00:44:14 2021 +0100\n\n    Further work on dataframe visualisations\n\ncommit 6d6499ea8976930d485aee7d50cbd454912c00d2\nAuthor: Stijn Denissen &lt;stijn.denissen@vub.be&gt;\nDate:   Sun Mar 21 00:46:44 2021 +0100\n\n    Remove errors and cell outputs\n\ncommit b845d510190ec111d71e9e419c6b138944b1edf6\nMerge: 5c0bdf2 6d6499e\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Sun Mar 21 16:21:42 2021 +0100\n\n    Merge pull request #8 from Sdniss/development\n    \n    Development\n\ncommit 172caa286b40b3911ef412e73fe952c6418b239a\nMerge: b845d51 da26220\nAuthor: Stijn Denissen &lt;40318470+Sdniss@users.noreply.github.com&gt;\nDate:   Sun Mar 21 16:22:02 2021 +0100\n\n    Merge pull request #6 from Remi-Gau/remi-dev\n    \n    [ENH] regorganize, rename notebooks and improve octave viz\n\ncommit 7ec9968d906dd7aac1af40b37d17cb260d32b232\nAuthor: Stijn Denissen &lt;stijn.denissen@vub.be&gt;\nDate:   Sun Mar 21 23:55:29 2021 +0100\n\n    Add integrative exercise + suggestions other plots\n\ncommit 760531a9658a821b9d8c4afdaaa3060755663644\nAuthor: Stijn Denissen &lt;stijn.denissen@vub.be&gt;\nDate:   Sun Mar 21 23:56:02 2021 +0100\n\n    Refine show_slice function + add docstrings\n\ncommit 3d0d7d42a7be53f60ef443d98dc7f9125efa89da\nAuthor: Stijn Denissen &lt;stijn.denissen@vub.be&gt;\nDate:   Mon Mar 22 12:19:09 2021 +0100\n\n    Rename notebook + add Nilearn section + code cleanup\n\ncommit 69611d1528de2a0222907c3606cab031b90bdafe\nAuthor: Stijn Denissen &lt;stijn.denissen@vub.be&gt;\nDate:   Tue Mar 23 18:41:21 2021 +0100\n\n    Layout adaptations\n\ncommit 29755cc265a6953dea4c67e298a94135ce48f892\nMerge: 172caa2 69611d1\nAuthor: Remi Gau &lt;remi_gau@hotmail.com&gt;\nDate:   Tue Mar 23 18:52:42 2021 +0100\n\n    Merge pull request #10 from Sdniss/main\n    \n    Python notebook finalisation\n\ncommit 409cb18e48d4d7c5614ab78a67157df6475eaca4\nAuthor: Stijn Denissen &lt;40318470+Sdniss@users.noreply.github.com&gt;\nDate:   Tue Mar 23 19:20:14 2021 +0100\n\n    Update README.md\n    \n    Fix binder link\n\ncommit 70e31100dc319367101fbe1d5c08c3150e1e5477\nAuthor: marianne-aspbury &lt;marianne.aspbury@pmb.ox.ac.uk&gt;\nDate:   Wed Mar 24 12:35:24 2021 +0000\n\n    Initial cleanup\n    \n    Removing data viz files and dependencies not necessary to example. Can always add more back\n\ncommit a22da50089792759f0cbefedfc56f4971f55ae2e\nAuthor: marianne-aspbury &lt;marianne.aspbury@pmb.ox.ac.uk&gt;\nDate:   Wed Mar 24 12:36:36 2021 +0000\n\n    Basic example\n    \n    Basic example from Remi Gau\n\ncommit 4851656c454d0048036a7df5d00ce98b5f1f0f1f\nAuthor: marianne-aspbury &lt;marianne.aspbury@pmb.ox.ac.uk&gt;\nDate:   Wed Mar 24 12:49:44 2021 +0000\n\n    set-up and added binder link\n\ncommit 9e62d97c32d0727955e20f957a4b63bbb4350996\nAuthor: marianne-aspbury &lt;marianne.aspbury@pmb.ox.ac.uk&gt;\nDate:   Wed Mar 24 13:14:03 2021 +0000\n\n    add tree package to install on virtual binder machine\n\ncommit 58c170b2fa1c76a8c6fc60891d7473e62f275d92\nAuthor: Yu-Fang Yang &lt;33165978+ufangYang@users.noreply.github.com&gt;\nDate:   Wed Mar 24 18:40:22 2021 +0100\n\n    remove txts to README\n\ncommit a957cb37122d1428c3442ce26d7f318f119ba72c\nAuthor: Yu-Fang Yang &lt;33165978+ufangYang@users.noreply.github.com&gt;\nDate:   Wed Mar 24 18:40:43 2021 +0100\n\n    move txt from contributing file\n\ncommit 06ac298f2f32ea2e552e955d77ea0cba3921a67e\nMerge: 9e62d97 a957cb3\nAuthor: marianne-aspbury &lt;37187502+marianne-aspbury@users.noreply.github.com&gt;\nDate:   Wed Mar 24 17:42:43 2021 +0000\n\n    Merge pull request #1 from ufangYang/main\n    \n    migrate txts from contributing file to README\n\ncommit ccd949d150505670185ad5fbf36306cdb9e93db8\nAuthor: marianne-aspbury &lt;marianne.aspbury@pmb.ox.ac.uk&gt;\nDate:   Wed Mar 24 17:49:43 2021 +0000\n\n    file clean\n    \n    remove not needed contributing file and tweak readme\n\ncommit 9dbedae1bbed99f89dbfd1c0794aafc001201731\nAuthor: jsheunis &lt;jsheunis@gmail.com&gt;\nDate:   Thu Apr 8 11:40:54 2021 +0200\n\n    first commit, adds converted+updated OHBM notebook\n\ncommit 2c990582f09ae734bc69b638bb2c53d259530833\nAuthor: jsheunis &lt;jsheunis@gmail.com&gt;\nDate:   Thu Apr 8 12:33:38 2021 +0200\n\n    split command line cells\n\ncommit e3da9ba56ae9d6078301a5e3407650876cbf06d2\nAuthor: jsheunis &lt;jsheunis@gmail.com&gt;\nDate:   Thu Apr 8 13:43:49 2021 +0200\n\n    command line cell tweaks\n\ncommit 0b188696522445baa48b69db2fe68e8f7821759e\nAuthor: jsheunis &lt;jsheunis@gmail.com&gt;\nDate:   Thu Apr 8 14:17:40 2021 +0200\n\n    remove kernel spec\n\ncommit 881aec1373214e2ceedfba9e6ab6432712e1e0bb\nAuthor: jsheunis &lt;jsheunis@gmail.com&gt;\nDate:   Thu Apr 8 14:38:14 2021 +0200\n\n    update readme\n\ncommit 4865c988bf2593f8e18ff0d1b15e891b50876c7e\nAuthor: jsheunis &lt;jsheunis@gmail.com&gt;\nDate:   Thu Apr 8 14:39:41 2021 +0200\n\n    fix typos\n\ncommit 30451c0184ad0b14b17ce0ff24e9b16f5bde09a1\nAuthor: Stephan Heunis &lt;s.heunis@fz-juelich.de&gt;\nDate:   Mon Jun 14 15:38:32 2021 +0200\n\n    update notebook; remove unnecessary packages from yml; add sklearn\n\ncommit b406c04eac77186ee7917f31a4d847397939f616\nAuthor: Stephan Heunis &lt;s.heunis@fz-juelich.de&gt;\nDate:   Wed Jun 16 11:27:05 2021 +0200\n\n    add asyncio workaround, update notebook\n\ncommit 56b2a44db96f7b8fcfa4bbb87a5da3b1899f13fb\nAuthor: Stephan Heunis &lt;s.heunis@fz-juelich.de&gt;\nDate:   Mon Sep 6 15:33:24 2021 +0200\n\n    remove unnecessary requirements file\n\ncommit e2c63ad11ab9e5a3cc1d1710eb5ada1df7fc16f6\nAuthor: Stephan Heunis &lt;s.heunis@fz-juelich.de&gt;\nDate:   Mon Sep 6 15:36:06 2021 +0200\n\n    add and install bash kernel for jupyter\n\ncommit a010714d22e97a0fc4e88ad8c9ae26ee1388f6ea\nAuthor: Stephan Heunis &lt;s.heunis@fz-juelich.de&gt;\nDate:   Mon Sep 6 15:39:36 2021 +0200\n\n    remove outdated readme comments\n\ncommit 90215a34e6f8e239ad251f24220195ee1a34dae7\nAuthor: Stephan Heunis &lt;s.heunis@fz-juelich.de&gt;\nDate:   Mon Sep 6 15:54:44 2021 +0200\n\n    adds bash-kernel notebook, to be edited\n\ncommit ee39736696adc550e13e1f6123a41790599c25fc\nAuthor: Stephan Heunis &lt;s.heunis@fz-juelich.de&gt;\nDate:   Mon Sep 6 15:58:51 2021 +0200\n\n    magic removed from bash notebook\n\ncommit 1052bde5bd9311211793591a4787c5bd0a41b0d0\nAuthor: Stephan Heunis &lt;s.heunis@fz-juelich.de&gt;\nDate:   Mon Sep 6 16:01:19 2021 +0200\n\n    change launch location\n\ncommit 5b9a383f40f177bcc500872c942fb3d00cc4dce4\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Sat Jun 21 17:59:04 2025 +0200\n\n    Update notebook\n\ncommit c593aca94085806942032165bce081c4d3da6e33\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Sat Jun 21 17:59:16 2025 +0200\n\n    Update README.md\n\ncommit 7c165a91aa923cc479f56c139b9e862cecc6a090\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Sat Jun 21 18:05:19 2025 +0200\n\n    Rename file\n\ncommit 4a74960bca2548e7202fdf4a9a63941ba237c744\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Sat Jun 21 18:08:33 2025 +0200\n\n    Delete files\n\ncommit f324bc05394e573b898d2e229c021cb5eb952896\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Sat Jun 21 22:52:41 2025 +0200\n\n    Update notebook\n\ncommit 94f552d92b07439253d75635e4eb2c0603f61b33\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Sat Jun 21 22:52:55 2025 +0200\n\n    Add requirements.in\n\ncommit eaee4fee95dad5314400bb7ddb949614dc46a7bd\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Mon Jul 7 20:29:52 2025 +0200\n\n    Update datalad tutorial\n\ncommit 29af18c4d7f15253429fe3fa8f132b600e7e0a61\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Mon Jul 7 21:35:47 2025 +0200\n\n    Minor fixes\n\ncommit a37d86b53e3ea47d8a174f7dc496585e92e42e9e\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Mon Jul 7 21:58:14 2025 +0200\n\n    Fix rerun demo\n\ncommit 69eb4ccf4c5cdf0c11b0de25f1e5dab83ef7f73a\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Mon Jul 7 23:50:51 2025 +0200\n\n    Play audio via VLC\n\ncommit 7079e4d3661a973f045d12751096be12a30cf24d\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Tue Jul 8 00:03:35 2025 +0200\n\n    Remove download of data\n\ncommit db0c101155ed1f7f7e2769c9d6f7e2e2928a423a\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Tue Jul 8 00:03:54 2025 +0200\n\n    Install mpg123\n\ncommit 8dfea55bb5810c8c1fcf6996a59cf5288a3b3b1d\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Tue Jul 8 00:14:44 2025 +0200\n\n    Revert \"Install mpg123\"\n    \n    This reverts commit db0c101155ed1f7f7e2769c9d6f7e2e2928a423a.\n\ncommit 72e262595948ca357a5daa25f2f90b2a35173892 (origin/main)\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Tue Jul 8 07:21:36 2025 +0200\n\n    Comment audio playback\n\ncommit 43ae32234908838654730841d73a1acc19656711\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Thu Feb 5 23:12:34 2026 +0100\n\n    Add uv\n\ncommit 1ebb3ccc4cacc611262cf89cd9b1c96e5e8305fa\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Thu Feb 5 23:14:45 2026 +0100\n\n    Add packages with uv\n\ncommit 4531e28921a60e7fc8a33e7c49581738ecc7c82e\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Thu Feb 5 23:15:03 2026 +0100\n\n    Add pre-commit\n\ncommit 077a583ed3dad7676ae3e50228884116e2d9fa8f\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Thu Feb 5 23:17:54 2026 +0100\n\n    Delete requirements.in\n\ncommit 10e946d9de8a98c2cdb8a01b8f8970a3dfdef7bd\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Thu Feb 5 23:19:16 2026 +0100\n\n    Add nbclient\n\ncommit 076a4d9d34d24dc6d30bf8947cd17e96ae3e2225\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Thu Feb 5 23:27:20 2026 +0100\n\n    Add Dockerfile and CI for Docker\n\ncommit 5d8c0270758161e473b76463cb28bdeac7740791\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Thu Feb 5 23:32:39 2026 +0100\n\n    Add basic Quarto setup\n\ncommit b4d6e12f1deb823f7bb76eec9cc440d2cfb26fea\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Thu Feb 5 23:33:08 2026 +0100\n\n    Add codespell\n\ncommit 8ca2c4b3e5b8eae0bd06f5b0ac7b318bfd53ce26\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Thu Feb 5 23:37:41 2026 +0100\n\n    Remove divs\n\ncommit ed251d792bf109b84745d691cdba0cc0add14226\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Thu Feb 5 23:38:52 2026 +0100\n\n    Remove hide-output from code cells\n\ncommit 21758e637d30d16a1892fff7fda03dc2665bafef\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Thu Feb 5 23:39:21 2026 +0100\n\n    Add #| output-fold: true\n\ncommit cf6589b4b7dd8583ff8e5a65b18f18b4a6648d31\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Thu Feb 5 23:41:17 2026 +0100\n\n    Add configs for ci and ipynb\n\ncommit 264c16c3abfa86f509be779a34f61696e8b1e829\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Thu Feb 5 23:45:39 2026 +0100\n\n    Push lnnrtwttkhn/rdm-datalad\n\ncommit 3fd9ca12d1e52d6dfacd0279f33fc26b6c3a469f\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Thu Feb 5 23:49:36 2026 +0100\n\n    Rebuild on changes to .github/workflows/docker.yml\n\ncommit a48769504896ec7955c382d276b357345444d062\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Thu Feb 5 23:55:02 2026 +0100\n\n    Remove headings\n\ncommit c4c3003dec229666103c43c9987b1ddd68523823\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Thu Feb 5 23:55:15 2026 +0100\n\n    Ignore *_ipynb\n\ncommit 1789c16389164f832a69fd08d63eb75af14083c4\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Thu Feb 5 23:58:32 2026 +0100\n\n    Always pull image from current branch\n\ncommit c1a5838c91139b42dabd96ad73e37b285e825e83\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Fri Feb 6 00:02:13 2026 +0100\n\n    Add info on Python API\n\ncommit 1001d9f0212667d307fa90ea6d05473276bd5bdd\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Fri Feb 6 00:04:52 2026 +0100\n\n    Use collapse-output\n\ncommit c95363cb28503ba3fdb5a30b1a7647464a22ad5d\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Fri Feb 6 00:09:12 2026 +0100\n\n    Add Git config\n\ncommit 38a52eeb3f066982e18ae4246242a3d5601e21b1\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Fri Feb 6 00:09:19 2026 +0100\n\n    Improve language\n\ncommit cbeb86c0b3ce96b139c25a6fca750ea384ad5799\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Fri Feb 6 00:10:04 2026 +0100\n\n    Fix trailing whitespace\n\ncommit 305084c7f21bbe18cd7331354edb1d475fe14ab2\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Fri Feb 6 00:11:58 2026 +0100\n\n    Remove binder\n\ncommit 37e29b8bdf0f254bff39bb57cb83f37f18c8061d\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Fri Feb 6 00:16:17 2026 +0100\n\n    Fix Makefile\n\ncommit 2fed3397eac06fb579bb7240ca36956c7793f50b\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Fri Feb 6 00:16:32 2026 +0100\n\n    Add documentation\n\ncommit 196194d724af9744a84b65c6e7dba6f6ac0cba9d\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Fri Feb 6 00:22:28 2026 +0100\n\n    Add git-annex\n\ncommit e6abe589c2443e989b0d6b384ab35687306dfca0 (HEAD -&gt; quarto, origin/quarto)\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Fri Feb 6 00:22:37 2026 +0100\n\n    Fix docs\n\n\n\nHow does this look in the top-level dataset? If we query the history of DataLad-101, there will be no commits related to MP3 files or any of the commits we have seen in the subdataset. Instead, we can see that the superdataset recorded the recordings/longnow dataset as a subdataset. This means it recorded where this dataset came from and what version it is in.\n\ncd ../../\n\n\ngit log -n 1\nCode Output\n\nfatal: not a git repository (or any of the parent directories): .git\n\n\nCode Output\n\n: 128\n\n\n\nThe subproject commit registered the most recent commit of the subdataset, and thus the subdataset version:\n\ncd recordings/longnow\nCode Output\n\nbash: cd: recordings/longnow: No such file or directory\n\n\nCode Output\n\n: 1\n\n\n\n\ngit log --oneline\nCode Output\n\nfatal: not a git repository (or any of the parent directories): .git\n\n\nCode Output\n\n: 128\n\n\n\n\ncd ../../"
  },
  {
    "objectID": "tutorial.html#more-on-data-versioning-nesting-and-a-glimpse-into-reproducible-paper",
    "href": "tutorial.html#more-on-data-versioning-nesting-and-a-glimpse-into-reproducible-paper",
    "title": "Version Control of Data with DataLad",
    "section": "More on data versioning, nesting, and a glimpse into reproducible paper",
    "text": "More on data versioning, nesting, and a glimpse into reproducible paper\nWe’ll clone a repository for a paper that shares manuscript, code, and data:\n\ncd ../\n\n\ndatalad clone https://github.com/psychoinformatics-de/paper-remodnav.git\nCode Output\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n\nCloning:   0%|                             | 0.00/2.00 [00:00&lt;?, ? candidates/s]\n\n\nEnumerating: 0.00 Objects [00:00, ? Objects/s]\n\n\n                                              \n\n\nCounting:   0%|                                | 0.00/802 [00:00&lt;?, ? Objects/s]\n\n\n                                                                                \n\n\nCompressing:   0%|                             | 0.00/375 [00:00&lt;?, ? Objects/s]\n\n\n                                                                                \n\n\nReceiving:   0%|                             | 0.00/2.11k [00:00&lt;?, ? Objects/s]\n\n\nReceiving:  19%|████▏                 | 401/2.11k [00:00&lt;00:00, 3.00k Objects/s]\n\n\nReceiving:  34%|███████▍              | 718/2.11k [00:00&lt;00:00, 1.88k Objects/s]\n\n\nReceiving:  53%|██████████▌         | 1.12k/2.11k [00:00&lt;00:00, 2.53k Objects/s]\n\n\n                                                                                \n\n\nResolving:   0%|                              | 0.00/1.09k [00:00&lt;?, ? Deltas/s]\n\n\n                                                                                \n                                                                                \ninstall(error): /paper-remodnav (dataset) [No working git-annex installation of version &gt;= 10.20230126. Visit http://handbook.datalad.org/r.html?install for instructions on how to install DataLad and git-annex.] [No working git-annex installation of version &gt;= 10.20230126. Visit http://handbook.datalad.org/r.html?install for instructions on how to install DataLad and git-annex.]\n\n\n\n\nCode Output\n\n: 1\n\n\n\nThe top-level dataset has many subdatasets. One of them, remodnav, is a dataset that contains the source code for a Python package called remodnav used in eye-tracking analyses:\n\ncd paper-remodnav\nCode Output\n\nbash: cd: paper-remodnav: No such file or directory\n\n\nCode Output\n\n: 1\n\n\n\n\ndatalad subdatasets\nCode Output\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n[ERROR  ] datalad.support.exceptions.NoDatasetFound(No dataset found at '/' for the purpose 'report on subdataset(s)'.  Specify a dataset to work with by providing its path via the `dataset` option, or change the current working directory to be in a dataset.) (NoDatasetFound) \n\nusage: datalad subdatasets [-h] [-d DATASET] [--state {present|absent|any}]\n\n                           [--fulfilled FULFILLED] [-r] [-R LEVELS]\n\n                           [--contains PATH] [--bottomup]\n\n                           [--set-property NAME VALUE]\n\n                           [--delete-property NAME] [--version]\n\n                           [PATH ...]\n\n\n\n\nCode Output\n\n: 2\n\n\n\nAfter cloning a dataset, its subdatasets will be recognized, but just as content is not yet retrieved for files in datasets, the subdatasets of datasets are not yet installed. If we navigate into an uninstalled subdataset, it will appear as an empty directory.\n\ncd remodnav\nCode Output\n\nbash: cd: remodnav: No such file or directory\n\n\nCode Output\n\n: 1\n\n\n\n\nls\nCode Output\n\napp  boot  etc   lib    media  opt   root  sbin  sys  usr\nbin  dev   home  lib64  mnt    proc  run   srv   tmp  var\n\n\n\nIn order to install a subdataset, we use datalad get with the --recursive flag:\n\ndatalad get --recursive --recursion-limit 2 -n .\nCode Output\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n[ERROR  ] datalad.support.exceptions.NoDatasetFound(No dataset found at '/' for the purpose \"get content of ['.']\".  Specify a dataset to work with by providing its path via the `dataset` option, or change the current working directory to be in a dataset.) (NoDatasetFound) \n\nusage: datalad get [-h] [-s LABEL] [-d PATH] [-r] [-R LEVELS] [-n]\n\n                   [-D DESCRIPTION] [--reckless [auto|ephemeral|shared-...]]\n\n                   [-J NJOBS] [--version]\n\n                   [PATH ...]\n\n\n\n\nCode Output\n\n: 2\n\n\n\n\nls\nCode Output\n\napp  boot  etc   lib    media  opt   root  sbin  sys  usr\nbin  dev   home  lib64  mnt    proc  run   srv   tmp  var\n\n\n\nThis command not only retrieves file contents, but it also installs subdatasets. So, if you want to be really lazy, just run datalad get --recursive -n in the root of a dataset to install all available subdatasets. The -n option prevents get from downloading any data, so only the subdatasets are installed without any data being downloaded. Here, the depth of recursion is limited. For one, it would take a while to install all subdatasets, but the very raw eye-tracking dataset contains subject IDs that should not be shared. Therefore, this subdataset is not accessible. If you try to install all subdatasets, the source eye-tracking data will throw an error, because it is not made publicly available.\nAfterwards, you can see that the remodnav subdataset also contains further subdatasets. In this case, these subdatasets contain data used for testing and validating software performance.\n\ndatalad subdatasets\nCode Output\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n[ERROR  ] datalad.support.exceptions.NoDatasetFound(No dataset found at '/' for the purpose 'report on subdataset(s)'.  Specify a dataset to work with by providing its path via the `dataset` option, or change the current working directory to be in a dataset.) (NoDatasetFound) \n\nusage: datalad subdatasets [-h] [-d DATASET] [--state {present|absent|any}]\n\n                           [--fulfilled FULFILLED] [-r] [-R LEVELS]\n\n                           [--contains PATH] [--bottomup]\n\n                           [--set-property NAME VALUE]\n\n                           [--delete-property NAME] [--version]\n\n                           [PATH ...]\n\n\n\n\nCode Output\n\n: 2\n\n\n\nOne of the validation data subdatasets came from another lab that shared their data. After the researchers were almost finished with their paper, they found another paper that reported a mistake in this data. The mistake was still present in the data they were using. By inspecting the history of this dataset, you can see that at one point, they contributed a fix that changed the data.\n\ncd remodnav/tests/data/anderson_etal\nCode Output\n\nbash: cd: remodnav/tests/data/anderson_etal: No such file or directory\n\n\nCode Output\n\n: 1\n\n\n\n\ngit log -n 3\nCode Output\n\nfatal: not a git repository (or any of the parent directories): .git\n\n\nCode Output\n\n: 128\n\n\n\nBecause DataLad can link subdatasets to precise versions, it is possible to consciously decide and openly record which version of the data is used. It is also possible to test how much results change by resetting the subdataset to an earlier state or updating the dataset to a more recent version."
  },
  {
    "objectID": "tutorial.html#full-provenance-capture-and-reproducibility",
    "href": "tutorial.html#full-provenance-capture-and-reproducibility",
    "title": "Version Control of Data with DataLad",
    "section": "Full provenance capture and reproducibility",
    "text": "Full provenance capture and reproducibility\nDataLad allows you to capture full provenance (i.e., a record that describes entities and processes that were involved in producing or influencing a digital resource): The origin of datasets, the origin of files obtained from web sources, complete machine-readable and automatically reproducible records of how files were created (including software environments). You or your collaborators can thus re-obtain or reproducibly recompute content with a single command, and make use of extensive provenance of dataset content (who created it, when, and how?).\n\n\ncd ../../../../../../\n\nFirst, create a new dataset, in this case with the yoda configuration:\n\ndatalad create -c yoda myanalysis\nCode Output\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n[ERROR  ] No working git-annex installation of version &gt;= 10.20230126. Visit http://handbook.datalad.org/r.html?install for instructions on how to install DataLad and git-annex. \n\n\n\n\nCode Output\n\n: 1\n\n\n\nThis sets up a helpful structure for the dataset with a code directory and some README files, and applies helpful configurations:\n\ncd myanalysis\n\n\ntree\nCode Output\n\n.\n\n0 directories, 0 files\n\n\n\nRead more about the YODA principles and the YODA configuration in the section on YODA in the DataLad Handbook.\nNext, install the input data as a subdataset. For this, the DataLad developers created a DataLad dataset with the “iris” data and published it on GitHub. Here, we’re installing it into a directory named input.\n\ndatalad clone -d . https://github.com/datalad-handbook/iris_data.git input/\nCode Output\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n\nCloning:   0%|                             | 0.00/2.00 [00:00&lt;?, ? candidates/s]\n\n\nEnumerating: 0.00 Objects [00:00, ? Objects/s]\n\n\n                                              \n\n\nCounting:   0%|                               | 0.00/25.0 [00:00&lt;?, ? Objects/s]\n\n\n                                                                                \n\n\nCompressing:   0%|                            | 0.00/19.0 [00:00&lt;?, ? Objects/s]\n\n\n                                                                                \n\n\nReceiving:   0%|                              | 0.00/25.0 [00:00&lt;?, ? Objects/s]\n\n\n                                                                                \n\n\nResolving:   0%|                               | 0.00/3.00 [00:00&lt;?, ? Deltas/s]\n\n\n                                                                                \n                                                                                \ninstall(error): input (dataset) [No working git-annex installation of version &gt;= 10.20230126. Visit http://handbook.datalad.org/r.html?install for instructions on how to install DataLad and git-annex.] [No working git-annex installation of version &gt;= 10.20230126. Visit http://handbook.datalad.org/r.html?install for instructions on how to install DataLad and git-annex.]\n\n\n\n\nCode Output\n\n: 1\n\n\n\nThe last thing needed is code to run on the data and produce results. For this, here is a k-means classification analysis script written in Python. You can find more details about this analysis in the section on a YODA-compliant data analysis project.\n\ncat &lt;&lt; EOT &gt; code/script.py\n\nimport pandas as pd\nimport seaborn as sns\nimport datalad.api as dl\nfrom sklearn import model_selection\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report\n\ndata = \"input/iris.csv\"\n\n# make sure that the data are obtained (get will also install linked sub-ds!):\ndl.get(data)\n\n# prepare the data as a pandas dataframe\ndf = pd.read_csv(data)\nattributes = [\"sepal_length\", \"sepal_width\", \"petal_length\",\"petal_width\", \"class\"]\ndf.columns = attributes\n\n# create a pairplot to plot pairwise relationships in the dataset\nplot = sns.pairplot(df, hue='class', palette='muted')\nplot.savefig('pairwise_relationships.png')\n\n# perform a K-nearest-neighbours classification with scikit-learn\n# Step 1: split data in test and training dataset (20:80)\narray = df.values\nX = array[:,0:4]\nY = array[:,4]\ntest_size = 0.20\nseed = 7\nX_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y,\n                                                                    test_size=test_size,\n                                                                    random_state=seed)\n# Step 2: Fit the model and make predictions on the test dataset\nknn = KNeighborsClassifier()\nknn.fit(X_train, Y_train)\npredictions = knn.predict(X_test)\n\n# Step 3: Save the classification report\nreport = classification_report(Y_test, predictions, output_dict=True)\ndf_report = pd.DataFrame(report).transpose().to_csv('prediction_report.csv')\n\nEOT\nCode Output\n\nbash: code/script.py: No such file or directory\n\n\nCode Output\n\n: 1\n\n\n\nSo far the script is untracked:\n\ndatalad status\nCode Output\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\nnothing to save, working tree clean\n\n\n\nLet’s save it with a datalad save command:\n\ndatalad save -m \"Add script for kNN classification and plotting\" code/script.py\nCode Output\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n\n\n\ndatalad run\nThe challenge that DataLad helps accomplish is running this script in a way that links the script to the results it produces and the data it was computed from. We can do this with the datalad run command. In principle, it is simple. You start with a clean dataset:\n\ndatalad status\nCode Output\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\nnothing to save, working tree clean\n\n\n\nThen, give the command you would execute with datalad run, in this case python code/script.py. DataLad will take the command, run it, and save all of the changes in the dataset under the commit message specified with the -m option. Thus, it associates the script with the results.\nBut it can be even more helpful. Here, we also specify the input data that the command needs, and DataLad will retrieve the data beforehand. We also specify the output of the command. Specifying the outputs will allow us to rerun the command later and update any outdated results.\n\ndatalad run -m \"Analyze iris data with classification analysis\" \\\n--input \"input/iris.csv\" \\\n--output \"prediction_report.csv\" \\\n--output \"pairwise_relationships.png\" \\\n\"python3 code/script.py\"\nCode Output\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n[INFO   ] Making sure inputs are available (this may take some time) \n\nrun(error): /myanalysis (dataset) [Input did not match existing file: input/iris.csv]\n\n\n\n\nCode Output\n\n: 1\n\n\n\nDataLad creates a commit in the dataset history. This commit includes the commit message as a human-readable summary of what was done. It contains the produced output, and it has a machine-readable record that includes information on the input data, the results, and the command that was run to create this result.\n\ngit log -n 1\nCode Output\n\nfatal: your current branch 'master' does not have any commits yet\n\n\nCode Output\n\n: 128\n\n\n\n\n\ndatalad rerun\nThis machine-readable record is particularly helpful, because we can now instruct DataLad to rerun this command. This means we don’t have to memorize what we did, and people that we share the dataset with don’t need to ask how this result was produced. They can simply let DataLad tell them.\nThis is accomplished with the datalad rerun command. For this demonstration, we have prepared this analysis dataset and published it to GitHub at https://github.com/lnnrtwttkhn/datalad-tutorial-myanalysis.\n\ncd ../\n\n\ngit clone https://github.com/lnnrtwttkhn/datalad-tutorial-myanalysis analysis_clone\nCode Output\n\nCloning into 'analysis_clone'...\nremote: Enumerating objects: 37, done.        \nremote: Counting objects:   2% (1/37)        remote: Counting objects:   5% (2/37)        remote: Counting objects:   8% (3/37)        remote: Counting objects:  10% (4/37)        remote: Counting objects:  13% (5/37)        remote: Counting objects:  16% (6/37)        remote: Counting objects:  18% (7/37)        remote: Counting objects:  21% (8/37)        remote: Counting objects:  24% (9/37)        remote: Counting objects:  27% (10/37)        remote: Counting objects:  29% (11/37)        remote: Counting objects:  32% (12/37)        remote: Counting objects:  35% (13/37)        remote: Counting objects:  37% (14/37)        remote: Counting objects:  40% (15/37)        remote: Counting objects:  43% (16/37)        remote: Counting objects:  45% (17/37)        remote: Counting objects:  48% (18/37)        remote: Counting objects:  51% (19/37)        remote: Counting objects:  54% (20/37)        remote: Counting objects:  56% (21/37)        remote: Counting objects:  59% (22/37)        remote: Counting objects:  62% (23/37)        remote: Counting objects:  64% (24/37)        remote: Counting objects:  67% (25/37)        remote: Counting objects:  70% (26/37)        remote: Counting objects:  72% (27/37)        remote: Counting objects:  75% (28/37)        remote: Counting objects:  78% (29/37)        remote: Counting objects:  81% (30/37)        remote: Counting objects:  83% (31/37)        remote: Counting objects:  86% (32/37)        remote: Counting objects:  89% (33/37)        remote: Counting objects:  91% (34/37)        remote: Counting objects:  94% (35/37)        remote: Counting objects:  97% (36/37)        remote: Counting objects: 100% (37/37)        remote: Counting objects: 100% (37/37), done.        \nremote: Compressing objects:   4% (1/24)        remote: Compressing objects:   8% (2/24)        remote: Compressing objects:  12% (3/24)        remote: Compressing objects:  16% (4/24)        remote: Compressing objects:  20% (5/24)        remote: Compressing objects:  25% (6/24)        remote: Compressing objects:  29% (7/24)        remote: Compressing objects:  33% (8/24)        remote: Compressing objects:  37% (9/24)        remote: Compressing objects:  41% (10/24)        remote: Compressing objects:  45% (11/24)        remote: Compressing objects:  50% (12/24)        remote: Compressing objects:  54% (13/24)        remote: Compressing objects:  58% (14/24)        remote: Compressing objects:  62% (15/24)        remote: Compressing objects:  66% (16/24)        remote: Compressing objects:  70% (17/24)        remote: Compressing objects:  75% (18/24)        remote: Compressing objects:  79% (19/24)        remote: Compressing objects:  83% (20/24)        remote: Compressing objects:  87% (21/24)        remote: Compressing objects:  91% (22/24)        remote: Compressing objects:  95% (23/24)        remote: Compressing objects: 100% (24/24)        remote: Compressing objects: 100% (24/24), done.        \nReceiving objects:   2% (1/37)Receiving objects:   5% (2/37)Receiving objects:   8% (3/37)Receiving objects:  10% (4/37)Receiving objects:  13% (5/37)Receiving objects:  16% (6/37)Receiving objects:  18% (7/37)Receiving objects:  21% (8/37)Receiving objects:  24% (9/37)Receiving objects:  27% (10/37)Receiving objects:  29% (11/37)Receiving objects:  32% (12/37)Receiving objects:  35% (13/37)Receiving objects:  37% (14/37)Receiving objects:  40% (15/37)Receiving objects:  43% (16/37)Receiving objects:  45% (17/37)Receiving objects:  48% (18/37)Receiving objects:  51% (19/37)Receiving objects:  54% (20/37)Receiving objects:  56% (21/37)Receiving objects:  59% (22/37)Receiving objects:  62% (23/37)Receiving objects:  64% (24/37)Receiving objects:  67% (25/37)Receiving objects:  70% (26/37)Receiving objects:  72% (27/37)Receiving objects:  75% (28/37)Receiving objects:  78% (29/37)Receiving objects:  81% (30/37)Receiving objects:  83% (31/37)Receiving objects:  86% (32/37)Receiving objects:  89% (33/37)Receiving objects:  91% (34/37)Receiving objects:  94% (35/37)Receiving objects:  97% (36/37)Receiving objects: 100% (37/37)Receiving objects: 100% (37/37), 4.22 KiB | 4.22 MiB/s, done.\nResolving deltas:   0% (0/6)Resolving deltas:  16% (1/6)Resolving deltas:  33% (2/6)Resolving deltas:  50% (3/6)Resolving deltas:  66% (4/6)Resolving deltas:  83% (5/6)Resolving deltas: 100% (6/6)Resolving deltas: 100% (6/6), done.\nremote: Total 37 (delta 6), reused 37 (delta 6), pack-reused 0 (from 0)        \n\n\n\n\ncd analysis_clone\n\nWe can clone this repository and provide, for example, the checksum of the run command to the datalad rerun command. DataLad will read the machine-readable record of what was done and recompute the exact same thing.\n\ndatalad rerun 3bb049d\nCode Output\n\n\nIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n\n[INFO   ] run commit 3bb049d; (Analyze iris data...) \n\n[INFO   ] Making sure inputs are available (this may take some time) \n\n\nCloning:   0%|                             | 0.00/4.00 [00:00&lt;?, ? candidates/s]\n\n\nEnumerating: 0.00 Objects [00:00, ? Objects/s]\n\n\n                                              \n\n\nCounting:   0%|                               | 0.00/25.0 [00:00&lt;?, ? Objects/s]\n\n\n                                                                                \n\n\nCompressing:   0%|                            | 0.00/19.0 [00:00&lt;?, ? Objects/s]\n\n\n                                                                                \n\n\nReceiving:   0%|                              | 0.00/25.0 [00:00&lt;?, ? Objects/s]\n\n\n                                                                                \n\n\nResolving:   0%|                               | 0.00/3.00 [00:00&lt;?, ? Deltas/s]\n\n\n                                                                                \n                                                                                \ninstall(error): input (dataset) [No working git-annex installation of version &gt;= 10.20230126. Visit http://handbook.datalad.org/r.html?install for instructions on how to install DataLad and git-annex.] [No working git-annex installation of version &gt;= 10.20230126. Visit http://handbook.datalad.org/r.html?install for instructions on how to install DataLad and git-annex.]\n\n[ERROR  ] No working git-annex installation of version &gt;= 10.20230126. Visit http://handbook.datalad.org/r.html?install for instructions on how to install DataLad and git-annex. \n\n\n\n\nCode Output\n\n: 1\n\n\n\nThis allows others to easily rerun your computations. It also spares you the need to remember how you executed a script, and you can inquire about where the results came from.\n\ngit log pairwise_relationships.png\nCode Output\n\ncommit 3bb049dfdf42d5fd08e12b064a1eb8423951fad3 (HEAD -&gt; main, origin/main, origin/HEAD)\nAuthor: Lennart Wittkuhn &lt;lennart.wittkuhn@tutanota.com&gt;\nDate:   Mon Jul 7 21:54:32 2025 +0200\n\n    [DATALAD RUNCMD] Analyze iris data with classification analysis\n    \n    === Do not change lines below ===\n    {\n     \"chain\": [],\n     \"cmd\": \"python3 code/script.py\",\n     \"dsid\": \"a60bb21c-c42a-439a-aca4-9d450d33ae63\",\n     \"exit\": 0,\n     \"extra_inputs\": [],\n     \"inputs\": [\n      \"input/iris.csv\"\n     ],\n     \"outputs\": [\n      \"prediction_report.csv\",\n      \"pairwise_relationships.png\"\n     ],\n     \"pwd\": \".\"\n    }\n    ^^^ Do not change lines above ^^^\n\n\n\nDone! Thanks for coding along!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home: Research Data Management with DataLad",
    "section": "",
    "text": "NoteAcknowledgements\n\n\n\n\n\nThis tutorial was initially created by Adina Wagner for the 2020 OHBM Brainhack Traintrack session on DataLad. This notebook accompanies this tutorial video by Adina Wagner."
  },
  {
    "objectID": "index.html#what-is-datalad",
    "href": "index.html#what-is-datalad",
    "title": "Home: Research Data Management with DataLad",
    "section": "What is DataLad?",
    "text": "What is DataLad?\nDataLad is a data management multitool that can assist you in handling the entire life cycle of digital objects. It is a command-line tool, free and open source, and available for all major operating systems. In the command line, all operations begin with the general datalad command."
  },
  {
    "objectID": "index.html#tutorial-contents",
    "href": "index.html#tutorial-contents",
    "title": "Home: Research Data Management with DataLad",
    "section": "Tutorial contents",
    "text": "Tutorial contents\nThis tutorial covers:\n\nIntroduction and setup - Getting started with DataLad\nCreating a DataLad dataset - Basic dataset creation and management\nVersion control workflows - Managing data and code changes over time\nDataset consumption and nesting - Working with existing datasets and subdatasets\nDataset nesting - Advanced nested dataset structures\nMore on data versioning, nesting, and a glimpse into reproducible paper - Real-world examples\nFull provenance capture and reproducibility - Complete workflow tracking and replication"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Home: Research Data Management with DataLad",
    "section": "Getting started",
    "text": "Getting started\nTo start the tutorial:\n\nVisit the Setup page for installation instructions\nFollow along with the Tutorial for hands-on practice"
  },
  {
    "objectID": "index.html#key-features-of-datalad",
    "href": "index.html#key-features-of-datalad",
    "title": "Home: Research Data Management with DataLad",
    "section": "Key features of DataLad",
    "text": "Key features of DataLad\n\nVersion control for data - Track changes to datasets of any size\nData consumption - Install and manage datasets from remote sources\nReproducible workflows - Capture complete provenance of data processing\nCollaboration - Share and synchronize datasets across teams\nStorage flexibility - Work with data locally or on remote storage"
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Home: Research Data Management with DataLad",
    "section": "Resources",
    "text": "Resources\n\nDataLad Documentation\nDataLad Handbook\nDataLad on GitHub\nDataLad Installation Guide"
  }
]